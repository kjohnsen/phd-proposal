[
  {
    "objectID": "src/specific-aims.html#sec-aims-rationale",
    "href": "src/specific-aims.html#sec-aims-rationale",
    "title": "2  Specific Aims",
    "section": "Rationale",
    "text": "Rationale\nAs the importance of causal inference becomes increasingly recognized in neuroscience, the need for technology enabling precise manipulation of neural variables becomes apparent. Feedback control is an important class of such manipulations for its ability to increase inference power by reducing response variability. Widely used throughout the engineering disciplines, it has had a significant impact through a variety of techniques (e.g., voltage clamp, dynamic clamp) on cellular neuroscience. However, feedback control also has great potential at the mesoscale/systems level, potentially enabling researchers to unambiguously infer the downstream effects of circuit/population-level neural activity.\nFor a number of reasons, though, this potential has not been widely realized.  I posit that the main challenges to adoption rather include the complexity of implementing fast closed-loop experiments, the need to adapt the mature methods of control theory to the idiosyncratic constraints of systems neuroscience experiments, and the lack of established technical guidelines for applying feedback control to address complex scientific questions. The proposed work aims to begin to address these challenges, and thus strengthen the set of causal tools available to probe neural systems."
  },
  {
    "objectID": "src/specific-aims.html#aim-1-a-cloc-experiment-simulation-testbed",
    "href": "src/specific-aims.html#aim-1-a-cloc-experiment-simulation-testbed",
    "title": "2  Specific Aims",
    "section": "Aim 1: A CLOC experiment simulation testbed",
    "text": "Aim 1: A CLOC experiment simulation testbed\nOne significant obstacle to closed-loop optogenetic control (CLOC) experiments is the cost of acquiring and configuring compatible hardware-software systems. Moreover, the maintenance of animals or cell cultures inherent in lab experiments can slow the pace of developing novel techniques. In Aim 1, I attempt to address these obstacles by developing a simulation framework for easily prototyping CLOC experiments in silico, thus enabling faster, cheaper CLOC experiment design and method development. We demonstrate the software’s utility in different virtual experiments and provide it to the public as open-source software with thorough documentation."
  },
  {
    "objectID": "src/specific-aims.html#aim-2-multi-input-cloc",
    "href": "src/specific-aims.html#aim-2-multi-input-cloc",
    "title": "2  Specific Aims",
    "section": "Aim 2: Multi-input CLOC",
    "text": "Aim 2: Multi-input CLOC\nMulti-input CLOC—the simultaneous use of multiple light sources and/or opsins—is necessary for precise manipulation of neural systems. However, the basic control theory methods previously used for CLOC, while fast, do not take actuator constraints into account and are thus likely to be inadequate for multi-actuator problems. The field of control theory provides elegant, powerful—if slower—solutions to this class of problems, but applying them requires interdisciplinary expertise. In this aim I will translate more sophisticated model-based feedback control algorithms to the multi-input CLOC setting and assess their merits compared to limited, simpler methods in silico for real-time control of brain networks."
  },
  {
    "objectID": "src/specific-aims.html#aim-3-using-cloc-to-manipulate-latent-neural-dynamics",
    "href": "src/specific-aims.html#aim-3-using-cloc-to-manipulate-latent-neural-dynamics",
    "title": "2  Specific Aims",
    "section": "Aim 3: Using CLOC to manipulate latent neural dynamics",
    "text": "Aim 3: Using CLOC to manipulate latent neural dynamics\nTo our knowledge, CLOC has yet to be applied in answering complex systems neuroscience questions. In this aim, to pave the way for future in-vivo experiments that accomplish this, I propose to develop technical and conceptual guidelines as I control the latent dynamics of simulated neural populations. First, I will produce these virtual models by training recurrent spiking neural networks with state-of-the-art, biologically plausible methods—each differing in their degrees of brain-like architecture and training procedure complexity. I will then use the simulation testbed of Aim 1 to explore how control quality varies with recording, stimulation, and control parameters and the complexity and size of the system—thus giving researchers a tentative idea of the relative importance of each factor of CLOC. I will test the hypothesis that low-dimensional dynamics can be manipulated with similarly low-dimensional, rather than per-neuron, actuation."
  },
  {
    "objectID": "src/background.html#sec-cl-neuro",
    "href": "src/background.html#sec-cl-neuro",
    "title": "3  Background",
    "section": "Closed-loop control in neuroscience",
    "text": "Closed-loop control in neuroscience\nMesoscale neuroscience—on the level of populations of neurons, rather than the whole brain or individual cells—is currently undergoing a revolution fueled by advances in neural manipulation (1–8) and measurement (9–16) technologies as well as data analysis methods (17–22). These have yielded unprecedented datasets (23, 24) and insights into network activity and plasticity (25–29) Moreover, they enable novel experimental paradigms such as direct closed-loop control of neural activity (30–39). This closed-loop stimulation offers exciting prospects of intervention in processes that are too fast or unpredictable to control manually or with pre-defined stimulation, such as sensory information processing, motor planning, and oscillatory activity. Unlike other forms of closed-loop control using environmental/stimulus input (40), behavioral output (41) or neurofeedback training (8, 42), the direct control of neural activity itself provides more opportunities for revealing the downstream effects of that activity.\n\nClosed-loop control of neural activity can be implemented in an event-triggered sense (37–39)—enabling the experimenter to respond to discrete events of interest, such as the arrival of a traveling wave (43) or sharp wave-ripple (44)—or in a feedback sense (33–36), driving the system towards a reference state or trajectory. The latter has multiple advantages over open-loop control (delivery of a pre-defined stimulus): by rejecting exogenous inputs, noise, and disturbances, it reduces variability across time and across trials, allowing for finer-scale inference. Additionally, it can compensate for model mismatch as it actively reduces error, whereas an open-loop stimulus is defined and limited by imperfect models. Moreover, whereas traditional perturbation methods often include lesioning (45), unnatural silencing, or extreme stimulation, feedback control poses a more naturalistic alternative. Clamping the activity of a population of neurons to their baseline, for example, effectively shuts down information flow from those neurons without departing from typical firing rates."
  },
  {
    "objectID": "src/background.html#various-scales-and-tools-for-closed-loop-control",
    "href": "src/background.html#various-scales-and-tools-for-closed-loop-control",
    "title": "3  Background",
    "section": "Various scales and tools for closed-loop control",
    "text": "Various scales and tools for closed-loop control\nClosed-loop control of neural activity can be performed at multiple scales and with different sets of tools. At the smallest, sub-neuron scale, feedback control has yielded decades of fruitful research in the form of tools such as the voltage clamp (46) and dynamic clamp (47, 48), controlling electrical properties of the cell membrane. The frontiers of this small-scale neuroscience often involve scaling up to many neurons and scaling down to subcellular structures such as dendrites, but multi-electrode, in-vivo, intracellular recording methods are challenging (49–53). Optical tools—e.g., optogenetics and fluorescence microscopy—can circumvent the difficulties of working with electrodes at such small scales, but an optical approach is not yet feasible for this purpose. The obstacles lie mainly in recording technology: the kinetics of both voltage indicators (16) and intracellular calcium (54) are too slow to capture phenomena faster than a typical action potential.\nBy contrast, the current state of technology is ripe for innovating closed-loop control methods at larger scales of neural activity, from single neurons to populations and circuits. Several promising combinations of recording and stimulation modalities are possible and still relatively novel: electrode recording with optogenetic stimulation (33–35, 55), fluorescence microscopy with electrical stimulation (16), fluorescence microscopy with photostimulation (all-optical control) (36, 56–59), and fMRI with optionally transcranial photostimulation (60, 61). Each of these tool combinations has pros and cons in terms of spatial and temporal resolution, crosstalk (62), and degrees of freedom. A natural starting point for many neuroscientists is the first of these tool sets—electrode recording combined with optogenetics—since the two methods are so widely used, interfere little with each other (as long as metal electrodes are not directly illuminated (55, 62)), and allow for genetically targeted stimulation. I will henceforth refer to this combination as CLOC, following the convention established by previous works (34, 35)."
  },
  {
    "objectID": "src/background.html#sec-prev-work",
    "href": "src/background.html#sec-prev-work",
    "title": "3  Background",
    "section": "Previous work",
    "text": "Previous work\nThe proposed work builds on the work my lab and collaborators have done previously in developing CLOC. This includes pioneering work taking feedback control from the cellular to the network level by controlling population firing rates in vitro—first by (63), who reduce unnatural global bursting and conclude as a result that such bursting results from the lack of the exogenous input present in the live, awake cortex. Later, (64) used the same proportional error-driven controller in a demonstration of user-friendly real-time electrophysiology software.\nThese model-free control methods later saw further development and the introduction of optogenetic actuation. In the first known demonstration of optogenetic feedback control, (33) used bidirectional stimulation for fixed and slowly varying firing rate targets in vitro (Figure 3.1 (a)). The authors improved control methods by an introducing an integral term to the model-free controller and, in another first, demonstrated feedback control of firing rates in vivo by bringing CLOC to the anesthetized rat. (34) used PI control again, but developed a more principled approach to set estimation and control parameters and tracked dynamic firing rate trajectories down to a ~100-ms timescale (Figure 3.1 (b)).\nLater, in a direct precursor to the proposed work, (35) employed more sophisticated and scalable optimal feedback control methods that can be characterized as an adaptive linear quadratic regulator (LQR). Built on a state-space, linear dynamical system model, the controller estimates the latent state using Kalman filtering (65) and calculates in real-time the optimal control input based on the current error. The authors enable adaptivity by estimating a second latent state, added to the first, that represents a slowly varying bias or disturbance to the system—important especially in awake animal experiments, where dynamic brain state changes contribute to high per-trial variability (Figure 3.1 (c)).\n\n\n\n\n\n\n(a)\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\nFigure 3.1: Previous CLOC experiments. (A) Figure 2 from (33), demonstrating control clamping a cultured neuron to different firing rates. \\(U_C\\) refers to the control signal for channelrhodopsin2(H134R) (ChR2R), parametrizing 470-nm light delivery. \\(U_H\\) likewise parametrizes 590-nm light delivery to activate enhanced halorhodopsin-3.0 (eNpHR3.0). (B) Figure 2 from (34), outlining an in-vivo experiment setup, a control diagram showing how optical input is determined from the firing rate estimated in real time, and a strategy for tuning the controller. (C) Figure 4b from (35), showing how CLOC clamps the firing rate of a single thalamic neuron in the awake mouse over multiple trials, reducing response variability (as measured by the Fano factor)."
  },
  {
    "objectID": "src/background.html#unexplored-applications-for-network-level-control",
    "href": "src/background.html#unexplored-applications-for-network-level-control",
    "title": "3  Background",
    "section": "Unexplored applications for network-level control",
    "text": "Unexplored applications for network-level control\nWhile preliminary technological foundations for feedback control of neural activity have already been laid, CLOC has yet to be applied to network-level variables beyond population firing rates to further causal hypothesis testing.    Examples of these potential targets for control include the activity of different cell types (66–71); the type (72–74), frequency (75), amplitude (75), spike coherence (76, 77) and interactions (78, 79) of different oscillatory patterns (80); discrete phenomena such as bursts (81), sharp wave ripples (44), oscillatory bursts (82–86), traveling waves (36, 43, 87–90), or sleep spindles (91); and latent states describing neural dynamics (92–99), including those most predictive of behavior (20, 100, 101). In the proposed work, I focus on the latter for their success in generating computational hypotheses for brain function and in predicting behavior. In the following chapters, I describe plans to move towards in-vivo control of latent dynamics by developing CLOC infrastructure and algorithms and finally performing simulations and analysis in silico.\n\n\n\n\n\n\n\n1. L. Fenno, O. Yizhar, K. Deisseroth, The development and application of optogenetics. Annual Review of Neuroscience. 34, 389–412 (2011).\n\n\n2. J. S. Wiegert, M. Mahn, M. Prigge, Y. Printz, O. Yizhar, Silencing Neurons: Tools, Applications, and Experimental Constraints. Neuron. 95, 504–529 (2017).\n\n\n3. S. Sridharan, M. A. Gajowa, M. B. Ogando, U. K. Jagadisan, L. Abdeladim, M. Sadahiro, H. A. Bounds, W. D. Hendricks, T. S. Turney, I. Tayler, K. Gopakumar, I. A. Oldenburg, S. G. Brohawn, H. Adesnik, High-performance microbial opsins for spatially and temporally precise perturbations of large neuronal networks. Neuron. 110, 1139–1155.e6 (2022).\n\n\n4. J. Vierock, S. Rodriguez-Rozada, A. Dieter, F. Pieper, R. Sims, F. Tenedini, A. C. F. Bergs, I. Bendifallah, F. Zhou, N. Zeitzschel, J. Ahlbeck, S. Augustin, K. Sauter, E. Papagiakoumou, A. Gottschalk, P. Soba, V. Emiliani, A. K. Engel, P. Hegemann, J. S. Wiegert, BiPOLES is an optogenetic tool developed for bidirectional dual-color control of neurons. Nature Communications. 12, 1–20 (2021).\n\n\n5. H. Adesnik, L. Abdeladim, Probing neural codes with two-photon holographic optogenetics. Nature Neuroscience. 24, 1356–1366 (2021).\n\n\n6. G. Faini, C. Molinier, C. Telliez, C. Tourain, B. C. Forget, E. Ronzitti, V. Emiliani, Ultrafast Light Targeting for High-Throughput Precise Control of Neuronal Networks. bioRxiv, 2021.06.14.448315 (2021).\n\n\n7. B. L. Roth, DREADDs for Neuroscientists. Neuron. 89, 683–694 (2016).\n\n\n8. D. Eriksson, A. Schneider, A. Thirumalai, M. Alyahyay, B. de la Crompe, K. Sharma, P. Ruther, I. Diester, Multichannel optogenetics combined with laminar recordings for ultra-controlled neuronal interrogation. Nature Communications. 13, 985 (2022).\n\n\n9. N. A. Steinmetz, C. Aydin, A. Lebedeva, M. Okun, M. Pachitariu, M. Bauza, M. Beau, J. Bhagat, C. Böhm, M. Broux, S. Chen, J. Colonell, R. J. Gardner, B. Karsh, F. Kloosterman, D. Kostadinov, C. Mora-Lopez, J. O’Callaghan, J. Park, J. Putzeys, B. Sauerbrei, R. J. J. van Daal, A. Z. Vollan, S. Wang, M. Welkenhuysen, Z. Ye, J. T. Dudman, B. Dutta, A. W. Hantman, K. D. Harris, A. K. Lee, E. I. Moser, J. O’Keefe, A. Renart, K. Svoboda, M. Häusser, S. Haesler, M. Carandini, T. D. Harris, Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings. Science. 372 (2021), doi:10.1126/science.abf4588.\n\n\n10. J. H. Siegle, A. C. López, Y. A. Patel, K. Abramov, S. Ohayon, J. Voigts, Open Ephys: An open-source, plugin-based platform for multichannel electrophysiology. Journal of Neural Engineering. 14, 045003 (2017).\n\n\n11. P. Gutruf, J. A. Rogers, Implantable, wireless device platforms for neuroscience research. Current Opinion in Neurobiology. 50, 42–49 (2018).\n\n\n12. W. Göbel, F. Helmchen, In Vivo Calcium Imaging of Neural Network Function. Physiology. 22, 358–365 (2007).\n\n\n13. T. Knöpfel, C. Song, Optical voltage imaging in neurons: Moving from technology development to practical tool. Nature Reviews Neuroscience. 20, 719–727 (2019).\n\n\n14. K. Svoboda, R. Yasuda, Principles of Two-Photon Excitation Microscopy and Its Applications to Neuroscience. Neuron. 50, 823–839 (2006).\n\n\n15. A. Kazemipour, O. Novak, D. Flickinger, J. S. Marvin, A. S. Abdelfattah, J. King, P. M. Borden, J. J. Kim, S. H. Al-Abdullatif, P. E. Deal, E. W. Miller, E. R. Schreiter, S. Druckmann, K. Svoboda, L. L. Looger, K. Podgorski, Kilohertz frame-rate two-photon tomography. Nature Methods. 16, 778–786 (2019).\n\n\n16. J. Wu, Y. Liang, S. Chen, C. L. Hsu, M. Chavarha, S. W. Evans, D. Shi, M. Z. Lin, K. K. Tsia, N. Ji, Kilohertz two-photon fluorescence microscopy imaging of neural activity in vivo. Nature Methods. 17, 287–290 (2020).\n\n\n17. L. van der Maaten, G. Hinton, Visualizing Data using t-SNE. Journal of Machine Learning Research. 9, 2579–2605 (2008).\n\n\n18. G. J. Berman, D. M. Choi, W. Bialek, J. W. Shaevitz, Mapping the stereotyped behaviour of freely moving fruit flies. Journal of The Royal Society Interface. 11, 20140672 (2014).\n\n\n19. A. Mathis, P. Mamidanna, K. M. Cury, T. Abe, V. N. Murthy, M. W. Mathis, M. Bethge, DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning. Nature Neuroscience. 21, 1281–1289 (2018).\n\n\n20. O. G. Sani, H. Abbaspourazad, Y. T. Wong, B. Pesaran, M. M. Shanechi, Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification. Nature Neuroscience. 24, 140–149 (2021).\n\n\n21. O. Sporns, Graph theory methods: Applications in brain networks. Dialogues in Clinical Neuroscience. 20, 111–121 (2018).\n\n\n22. S. Schneider, J. H. Lee, M. W. Mathis, Learnable latent embeddings for joint behavioral and neural analysis (2022), doi:10.48550/arXiv.2204.00673.\n\n\n23. L. K. Scheffer, C. S. Xu, M. Januszewski, Z. Lu, S. Takemura, K. J. Hayworth, G. B. Huang, K. Shinomiya, J. Maitlin-Shepard, S. Berg, J. Clements, P. M. Hubbard, W. T. Katz, L. Umayam, T. Zhao, D. Ackerman, T. Blakely, J. Bogovic, T. Dolafi, D. Kainmueller, T. Kawase, K. A. Khairy, L. Leavitt, P. H. Li, L. Lindsey, N. Neubarth, D. J. Olbris, H. Otsuna, E. T. Trautman, M. Ito, A. S. Bates, J. Goldammer, T. Wolff, R. Svirskas, P. Schlegel, E. Neace, C. J. Knecht, C. X. Alvarado, D. A. Bailey, S. Ballinger, J. A. Borycz, B. S. Canino, N. Cheatham, M. Cook, M. Dreher, O. Duclos, B. Eubanks, K. Fairbanks, S. Finley, N. Forknall, A. Francis, G. P. Hopkins, E. M. Joyce, S. Kim, N. A. Kirk, J. Kovalyak, S. A. Lauchie, A. Lohff, C. Maldonado, E. A. Manley, S. McLin, C. Mooney, M. Ndama, O. Ogundeyi, N. Okeoma, C. Ordish, N. Padilla, C. M. Patrick, T. Paterson, E. E. Phillips, E. M. Phillips, N. Rampally, C. Ribeiro, M. K. Robertson, J. T. Rymer, S. M. Ryan, M. Sammons, A. K. Scott, A. L. Scott, A. Shinomiya, C. Smith, K. Smith, N. L. Smith, M. A. Sobeski, A. Suleiman, J. Swift, S. Takemura, I. Talebi, D. Tarnogorska, E. Tenshaw, T. Tokhi, J. J. Walsh, T. Yang, J. A. Horne, F. Li, R. Parekh, P. K. Rivlin, V. Jayaraman, M. Costa, G. S. Jefferis, K. Ito, S. Saalfeld, R. George, I. A. Meinertzhagen, G. M. Rubin, H. F. Hess, V. Jain, S. M. Plaza, A connectome and analysis of the adult Drosophila central brain. eLife. 9, e57443 (2020).\n\n\n24. A. L. Juavinett, G. Bekheet, A. K. Churchland, Chronically implanted Neuropixels probes enable high-yield recordings in freely moving mice. eLife. 8, e47188 (2019).\n\n\n25. E. R. Oby, M. D. Golub, J. A. Hennig, A. D. Degenhart, E. C. Tyler-Kabara, B. M. Yu, S. M. Chase, A. P. Batista, New neural activity patterns emerge with long-term learning. Proceedings of the National Academy of Sciences. 116, 15210–15215 (2019).\n\n\n26. Y. Yang, S. Qiao, O. G. Sani, J. I. Sedillo, B. Ferrentino, B. Pesaran, M. M. Shanechi, Modelling and prediction of the dynamic responses of large-scale brain networks during direct electrical stimulation. Nature Biomedical Engineering. 5, 324–345 (2021).\n\n\n27. B. R. Cowley, A. C. Snyder, K. Acar, R. C. Williamson, B. M. Yu, M. A. Smith, Slow Drift of Neural Activity as a Signature of Impulsivity in Macaque Visual and Prefrontal Cortex. Neuron. 108, 551–567.e8 (2020).\n\n\n28. L. Avitan, C. Stringer, Not so spontaneous: Multi-dimensional representations of behaviors and context in sensory areas. Neuron (2022), doi:10.1016/j.neuron.2022.06.019.\n\n\n29. M. Jazayeri, S. Ostojic, Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity. Current Opinion in Neurobiology. 70, 113–120 (2021).\n\n\n30. L. Grosenick, J. H. Marshel, K. Deisseroth, Review Closed-Loop and Activity-Guided Optogenetic Control. Neuron. 86, 106–139 (2015).\n\n\n31. A. Kumar, I. Vlachos, A. Aertsen, C. Boucsein, Challenges of understanding brain function by selective modulation of neuronal subpopulations. Trends in Neurosciences. 36, 579–586 (2013).\n\n\n32. S. M. Potter, A. El Hady, E. E. Fetz, Closed-loop neuroscience and neuroengineering. Frontiers in Neural Circuits. 0, 115 (2014).\n\n\n33. J. P. Newman, M. F. Fong, D. C. Millard, C. J. Whitmire, G. B. Stanley, S. M. Potter, Optogenetic feedback control of neural activity. eLife (2015), doi:10.7554/eLife.07192.\n\n\n34. M. F. Bolus, A. A. Willats, C. J. Whitmire, C. J. Rozell, G. B. Stanley, Design strategies for dynamic closed-loop optogenetic neurocontrol in vivo. Journal of Neural Engineering. 15, 026011 (2018).\n\n\n35. M. F. Bolus, A. A. Willats, C. J. Rozell, G. B. Stanley, State-space optimal feedback control of optogenetically driven neural activity. Journal of neural engineering. 18, 036006 (2021).\n\n\n36. Z. Zhang, L. E. Russell, A. M. Packer, O. M. Gauld, M. Häusser, Closed-loop all-optical interrogation of neural circuits in vivo. Nature Methods. 15, 1037–1040 (2018).\n\n\n37. E. Krook-Magnuson, C. Armstrong, M. Oijala, I. Soltesz, On-demand optogenetic control of spontaneous seizures in temporal lobe epilepsy. Nature Communications. 4, 1–8 (2013).\n\n\n38. A. Witt, A. Palmigiano, A. Neef, A. El Hady, F. Wolf, D. Battaglia, Controlling the oscillation phase through precisely timed closed-loop optogenetic stimulation: A computational study. Frontiers in Neural Circuits. 7, 1–17 (2013).\n\n\n39. S. Dutta, E. Ackermann, C. Kemere, Analysis of an open source, closed-loop, realtime system for hippocampal sharp-wave ripple disruption. Journal of Neural Engineering. 16, 016009 (2019).\n\n\n40. S. Tafazoli, C. J. MacDowell, Z. Che, K. C. Letai, C. R. Steinhardt, T. J. Buschman, Learning to control the brain through adaptive closed-loop patterned stimulation. Journal of Neural Engineering. 17, 056007 (2020).\n\n\n41. S. S. Srinivasan, B. E. Maimon, M. Diaz, H. Song, H. M. Herr, Closed-loop functional optogenetic stimulation. Nature Communications. 9, 1–10 (2018).\n\n\n42. M. Prsa, G. L. Galiñanes, D. Huber, Rapid Integration of Artificial Sensory Feedback during Operant Conditioning of Motor Cortex Neurons. Neuron. 93, 929–939.e6 (2017).\n\n\n43. Z. W. Davis, L. Muller, J. Martinez-Trujillo, T. Sejnowski, J. H. Reynolds, Spontaneous travelling cortical waves gate perception in behaving primates. Nature. 587, 432–436 (2020).\n\n\n44. G. Buzsáki, Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and planning. Hippocampus. 25, 1073–1188 (2015).\n\n\n45. A. R. Vaidya, M. S. Pujara, M. Petrides, E. A. Murray, L. K. Fellows, Lesion Studies in Contemporary Neuroscience. Trends in Cognitive Sciences. 23, 653–671 (2019).\n\n\n46. A. L. Hodgkin, A. F. Huxley, B. Katz, Measurement of current-voltage relations in the membrane of the giant axon of Loligo. The Journal of Physiology. 116, 424–448 (1952).\n\n\n47. A. A. Sharp, M. B. O’Neil, L. F. Abbott, E. Marder, The dynamic clamp: Artificial conductances in biological neurons. Trends in Neurosciences. 16, 389–394 (1993).\n\n\n48. A. A. Prinz, L. F. Abbott, E. Marder, The dynamic clamp comes of age. Trends in Neurosciences. 27, 218–224 (2004).\n\n\n49. J. T. Davie, M. H. P. Kole, J. J. Letzkus, E. A. Rancz, N. Spruston, G. J. Stuart, M. Häusser, Dendritic patch-clamp recording. Nature Protocols. 1, 1235–1247 (2006).\n\n\n50. G. Wang, D. R. Wyskiel, W. Yang, Y. Wang, L. C. Milbern, T. Lalanne, X. Jiang, Y. Shen, Q. Q. Sun, J. J. Zhu, An optogenetics- and imaging-assisted simultaneous multiple patch-clamp recording system for decoding complex neural circuits. Nature Protocols. 10, 397–412 (2015).\n\n\n51. D. Engel, Subcellular patch-clamp recordings from the somatodendritic domain of nigral dopamine neurons. Journal of Visualized Experiments. 2016, e54601 (2016).\n\n\n52. Y. Peng, F. X. Mittermaier, H. Planert, U. C. Schneider, H. Alle, J. R. P. Geiger, High-throughput microcircuit analysis of individual human brains through next-generation multineuron patch-clamp. eLife. 8, e48178 (2019).\n\n\n53. A. Alegria, A. Joshi, J. O’Brien, S. B. Kodandaramaiah, Single neuron recording: Progress towards high-throughput analysis. Bioelectronics in Medicine. 3, 33–36 (2020).\n\n\n54. M. Inoue, Genetically encoded calcium indicators to probe complex brain circuit dynamics in vivo. Neuroscience Research. 169, 2–8 (2021).\n\n\n55. J. A. Cardin, M. Carlén, K. Meletis, U. Knoblich, F. Zhang, K. Deisseroth, L. H. Tsai, C. I. Moore, Targeted optogenetic stimulation and recording of neurons in vivo using cell-type-specific expression of Channelrhodopsin-2. Nature Protocols. 5, 247–254 (2010).\n\n\n56. N. C. Flytzanis, C. N. Bedbrook, H. Chiu, M. K. M. Engqvist, C. Xiao, K. Y. Chan, P. W. Sternberg, F. H. Arnold, V. Gradinaru, Archaerhodopsin variants with enhanced voltage-sensitive fluorescence in mammalian and Caenorhabditis elegans neurons. Nature Communications. 5, 4894 (2014).\n\n\n57. D. R. Hochbaum, Y. Zhao, S. L. Farhi, N. Klapoetke, C. A. Werley, V. Kapoor, P. Zou, J. M. Kralj, D. MacLaurin, N. Smedemark-Margulies, J. L. Saulnier, G. L. Boulting, C. Straub, Y. K. Cho, M. Melkonian, G. K. S. Wong, D. J. Harrison, V. N. Murthy, B. L. Sabatini, E. S. Boyden, R. E. Campbell, A. E. Cohen, All-optical electrophysiology in mammalian neurons using engineered microbial rhodopsins. Nature Methods. 11, 825–833 (2014).\n\n\n58. V. Emiliani, A. E. Cohen, K. Deisseroth, M. Häusser, All-optical interrogation of neural circuits. Journal of Neuroscience. 35, 13917–13926 (2015).\n\n\n59. K. E. Kishi, Y. S. Kim, M. Fukuda, M. Inoue, T. Kusakizako, P. Y. Wang, C. Ramakrishnan, E. F. X. Byrne, E. Thadhani, J. M. Paggi, T. E. Matsui, K. Yamashita, T. Nagata, M. Konno, S. Quirin, M. Lo, T. Benster, T. Uemura, K. Liu, M. Shibata, N. Nomura, S. Iwata, O. Nureki, R. O. Dror, K. Inoue, K. Deisseroth, H. E. Kato, Structural basis for channel conduction in the pump-like channelrhodopsin ChRmine. Cell. 185, 672–689.e23 (2022).\n\n\n60. J. Y. Lin, P. M. Knutsen, A. Muller, D. Kleinfeld, R. Y. Tsien, ReaChR: A red-shifted variant of channelrhodopsin enables deep transcranial optogenetic excitation. Nature Neuroscience. 16, 1499–1508 (2013).\n\n\n61. S. Chen, A. Z. Weitemier, X. Zeng, L. He, X. Wang, Y. Tao, A. J. Y. Huang, Y. Hashimotodani, M. Kano, H. Iwasaki, L. K. Parajuli, S. Okabe, D. B. Loong Teh, A. H. All, I. Tsutsui-Kimura, K. F. Tanaka, X. Liu, T. J. McHugh, Near-infrared deep brain stimulation via upconversion nanoparticlemediated optogenetics. Science. 359, 679–684 (2018).\n\n\n62. A. M. Packer, B. Roska, M. Häuser, Targeting neurons and photons for optogenetics. Nature Neuroscience. 16, 805–815 (2013).\n\n\n63. D. A. Wagenaar, R. Madhavan, J. Pine, S. M. Potter, Controlling bursting in cortical cultures with closed-loop multi-electrode stimulation. Journal of Neuroscience. 25, 680–688 (2005).\n\n\n64. J. Newman, R. Zeller-Townson, M. Fong, S. Arcot Desai, R. Gross, S. Potter, Closed-Loop, Multichannel Experimentation Using the Open-Source NeuroRighter Electrophysiology Platform. Frontiers in Neural Circuits. 6 (2013).\n\n\n65. R. E. Kalman, A new approach to linear filtering and prediction problems. Journal of Fluids Engineering, Transactions of the ASME. 82, 35–45 (1960).\n\n\n66. E. A. Mukamel, J. Ngai, Perspectives on defining cell types in the brain. Current Opinion in Neurobiology. 56, 61–68 (2019).\n\n\n67. P. Němec, P. Osten, The evolution of brain structure captured in stereotyped cell count and cell type distributions. Current Opinion in Neurobiology. 60, 176–183 (2020).\n\n\n68. R. I. Martinez-Garcia, B. Voelcker, J. B. Zaltsman, S. L. Patrick, T. R. Stevens, B. W. Connors, S. J. Cruikshank, Two dynamically distinct circuits drive inhibition in the sensory thalamus. Nature. 583, 813–818 (2020).\n\n\n69. C. Lee, A. Lavoie, J. Liu, S. X. Chen, B. Liu, Light Up the Brain: The Application of Optogenetics in Cell-Type Specific Dissection of Mouse Brain Circuits. Frontiers in Neural Circuits. 14 (2020).\n\n\n70. A. Joglekar, A. Prjibelski, A. Mahfouz, P. Collier, S. Lin, A. K. Schlusche, J. Marrocco, S. R. Williams, B. Haase, A. Hayes, J. G. Chew, N. I. Weisenfeld, M. Y. Wong, A. N. Stein, S. A. Hardwick, T. Hunt, Q. Wang, C. Dieterich, Z. Bent, O. Fedrigo, S. A. Sloan, D. Risso, E. D. Jarvis, P. Flicek, W. Luo, G. S. Pitt, A. Frankish, A. B. Smit, M. E. Ross, H. U. Tilgner, A spatially resolved brain region- and cell type-specific isoform atlas of the postnatal mouse brain. Nature Communications. 12, 463 (2021).\n\n\n71. H. Zeng, What is a cell type and how to define it? Cell. 185, 2739–2755 (2022).\n\n\n72. S. R. Cole, B. Voytek, Brain Oscillations and the Importance of Waveform Shape. Trends in Cognitive Sciences. 21, 137–149 (2017).\n\n\n73. S. Cole, B. Voytek, Cycle-by-cycle analysis of neural oscillations. Journal of Neurophysiology. 122, 849–861 (2019).\n\n\n74. M. S. Fabus, A. J. Quinn, C. E. Warnaby, M. W. Woolrich, Automatic decomposition of electrophysiological data into distinct nonsinusoidal oscillatory modes. Journal of Neurophysiology. 126, 1670–1684 (2021).\n\n\n75. A. B. Saleem, A. D. Lien, M. Krumin, B. Haider, M. R. Rosón, A. Ayaz, K. Reinhold, L. Busse, M. Carandini, K. D. Harris, M. Carandini, Subcortical Source and Modulation of the Narrowband Gamma Oscillation in Mouse Visual Cortex. Neuron. 93, 315–322 (2017).\n\n\n76. E. A. Buffalo, P. Fries, R. Landman, T. J. Buschman, R. Desimone, Laminar differences in gamma and alpha coherence in the ventral stream. Proceedings of the National Academy of Sciences of the United States of America. 108, 11262–11267 (2011).\n\n\n77. T. J. Buschman, E. L. Denovellis, C. Diogo, D. Bullock, E. K. Miller, Synchronous Oscillatory Neural Ensembles for Rules in the Prefrontal Cortex. Neuron. 76, 838–846 (2012).\n\n\n78. J. Aru, J. Aru, V. Priesemann, M. Wibral, L. Lana, G. Pipa, W. Singer, R. Vicente, Untangling cross-frequency coupling in neuroscience. Current Opinion in Neurobiology. 31, 51–61 (2015).\n\n\n79. L. Zhang, J. Lee, C. Rozell, A. C. Singer, Sub-second dynamics of theta-gamma coupling in hippocampal CA1. eLife. 8 (2019), doi:10.7554/eLife.44320.\n\n\n80. G. Buzsáki, A. Draguhn, Neuronal oscillations in cortical networks. Science. 304, 1926–1929 (2004).\n\n\n81. W. Gerstner, W. M. Kistler, R. Naud, L. Paninski, Neuronal dynamics: From single neurons to networks and models of cognition (Cambridge University Press, 2014).\n\n\n82. T. Akam, D. M. Kullmann, Oscillatory multiplexing of population codes for selective communication in the mammalian brain. Nature Reviews Neuroscience. 15, 111–122 (2014).\n\n\n83. I. Tal, S. Neymotin, S. Bickel, P. Lakatos, C. E. Schroeder, Oscillatory Bursting as a Mechanism for Temporal Coupling and Information Coding. Frontiers in Computational Neuroscience. 14 (2020).\n\n\n84. M. Lundqvist, J. Rose, P. Herman, S. L. L. Brincat, T. J. J. Buschman, E. K. K. Miller, Gamma and Beta Bursts Underlie Working Memory. Neuron. 90, 152–164 (2016).\n\n\n85. M. Lundqvist, J. Rose, S. L. Brincat, M. R. Warden, T. J. Buschman, P. Herman, E. K. Miller, Reduced variability of bursting activity during working memory. Scientific Reports. 12, 15050 (2022).\n\n\n86. G. Karvat, A. Schneider, M. Alyahyay, F. Steenbergen, M. Tangermann, I. Diester, Real-time detection of neural oscillation bursts allows behaviourally relevant neurofeedback. Communications Biology. 3, 1–10 (2020).\n\n\n87. T. K. Sato, I. Nauhaus, M. Carandini, Traveling Waves in Visual Cortex. Neuron. 75, 218–229 (2012).\n\n\n88. L. Muller, F. Chavane, J. Reynolds, T. J. Sejnowski, Cortical travelling waves: Mechanisms and computational principles. Nature Reviews Neuroscience. 19, 255–268 (2018).\n\n\n89. M. E. Rule, C. Vargas-Irwin, J. P. Donoghue, W. Truccolo, Phase reorganization leads to transient \\(\\beta\\)-LFP spatial wave patterns in motor cortex during steady-state movement preparation. Journal of Neurophysiology. 119, 2212–2228 (2018).\n\n\n90. S. Moldakarimov, M. Bazhenov, D. E. Feldman, T. J. Sejnowski, Structured networks support sparse traveling waves in rodent somatosensory cortex. Proceedings of the National Academy of Sciences of the United States of America. 115, 5277–5282 (2018).\n\n\n91. L. M. J. Fernandez, A. Lüthi, Sleep Spindles: Mechanisms and Functions. Physiological Reviews. 100, 805–868 (2020).\n\n\n92. M. M. Churchland, J. P. Cunningham, M. T. Kaufman, J. D. Foster, P. Nuyujukian, S. I. Ryu, K. V. Shenoy, K. V. Shenoy, Neural population dynamics during reaching. Nature. 487, 51–56 (2012).\n\n\n93. K. V. Shenoy, M. Sahani, M. M. Churchland, Cortical Control of Arm Movements: A Dynamical Systems Perspective. Annual Review of Neuroscience. 36, 337–359 (2013).\n\n\n94. J. P. Cunningham, B. M. Yu, Dimensionality reduction for large-scale neural recordings. Nature neuroscience. 17, 1500–9 (2014).\n\n\n95. M. T. Kaufman, M. M. Churchland, S. I. Ryu, K. V. Shenoy, Cortical activity in the null space: Permitting preparation without movement. Nature Neuroscience. 17, 440–448 (2014).\n\n\n96. J. A. Gallego, M. G. Perich, L. E. Miller, S. A. Solla, Neural Manifolds for the Control of Movement. Neuron. 94, 978–984 (2017).\n\n\n97. S. Vyas, M. D. Golub, D. Sussillo, K. V. Shenoy, Computation through Neural Population Dynamics. Annual Review of Neuroscience. 43, 249–275 (2020).\n\n\n98. K. V. Shenoy, J. C. Kao, Measurement, manipulation and modeling of brain-wide neural population dynamics. Nature Communications. 12, 1–5 (2021).\n\n\n99. D. Peixoto, J. R. Verhein, R. Kiani, J. C. Kao, P. Nuyujukian, C. Chandrasekaran, J. Brown, S. Fong, S. I. Ryu, K. V. Shenoy, W. T. Newsome, Decoding and perturbing decision states in real time. Nature. 591, 604–609 (2021).\n\n\n100. O. G. Sani, B. Pesaran, M. M. Shanechi, M. Hsieh, Where is all the nonlinearity: Flexible nonlinear modeling of behaviorally relevant neural dynamics using recurrent neural networks. bioRxiv, 2021.09.03.458628 (2021).\n\n\n101. C. Hurwitz, A. Srivastava, K. Xu, J. Jude, M. G. Perich, L. E. Miller, M. H. Hennig, \"Targeted Neural Dynamical Modeling\" in Advances in Neural Information Processing Systems (2021; https://arxiv.org/abs/2110.14853), vol. 35, pp. 29379–29392."
  },
  {
    "objectID": "src/aim1.html#rationale",
    "href": "src/aim1.html#rationale",
    "title": "4  Aim 1 - A CLOC simulation testbed",
    "section": "Rationale",
    "text": "Rationale\nCLOC experiments can be difficult and costly, posing a barrier to entry for neuroscientists that find CLOC’s advantages attractive. Their lab might lack the funds or time to invest in hardware and software infrastructure or the confidence that such an investment would yield successful experiments. Moreover, when the proposed experiment requires signal processing/control method development, iterating on designs in-vivo may be cumbersome, given the additional cost of animal care and training. However, these costs and risks can be mitigated through in-silico prototyping. Given a reliable model of the system of interest, one can simulate an experiment and assess its effectiveness. Alternate models of a system can be tested to assess robustness of a method to unknown model properties, or that method can be validated on a variety of systems to determine its general applicability. This strategy not only allows for a researcher to evaluate and optimize methods before committing significant resources to them, but also accelerates the development cycle.\n\nInnovation\nFor these reasons, I have developed Cleo: Closed Loop, Electrophysiology, and Optogenetics experiment simulation testbed. Unlike existing software, Cleo simultaneously provides a high-level interface to fast and flexible neural network simulations; easy, model-independent injection of electrode recording and optogenetic perturbations; and a real-time, closed-loop processing module capable of modeling communication and processing delays inherent in real experiments. I thus provide a low-cost environment to design and develop CLOC methods, as well as for researchers to see if CLOC may serve their research agenda."
  },
  {
    "objectID": "src/aim1.html#approach",
    "href": "src/aim1.html#approach",
    "title": "4  Aim 1 - A CLOC simulation testbed",
    "section": "Approach",
    "text": "Approach\n\nGuiding principles\nTwo factors drove our choice of recording and stimulation models to integrate into Cleo. First, because a main purpose of Cleo is to enable prototyping of experiments, we focused on models at the level of the parameters an experimenter is able to alter—this led us to choose models embedded in physical, 3D space. Second, we assumed that highly realistic models of individual neurons are not needed to capture most mesoscale-level phenomena. Accordingly, Cleo was developed with the intention of using point neurons models, rather than multi-compartment neuron models with realistic morphologies.\nIn addition to our modeling priorities, the goals of usability and extensibility guided our choices in software dependencies and infrastructure. Ease of use is important to make Cleo as accessible as possible to researchers with varied backgrounds, and motivated Cleo’s modular design, which allows users to add different recording or stimulation devices with little or no modification to the underlying network model. Additionally, extensibility is important for the testbed to remain relevant under changing needs in the future, allowing for new functionality to be easily added in a “plug-in,” modular architecture.\n\n\nClosed-loop simulation architecture\n\n\n\nFigure 4.1: A conceptual diagram of Cleo’s functionality. Cleo wraps a Brian network model, injects stimulation and recording devices, and interfaces with the network in real time through a simulated “I/O processor”.\n\n\nWe chose Brian 2 (1) as the spiking neural network simulator to build Cleo around. Brian has the advantages of being flexible and intuitive, where models are defined with user-provided mathematical equations. Moreover, it is written in Python, an open-source, intuitive language (2) widely used in computational neuroscience (3, 4)—users do not need to use any other languages to use Brian and Cleo. Brian is also relatively fast (especially since it is developed primarily for point neuron simulations), as shown in benchmarks (1).\nCleo provides three modules—recording, stimulation, and closed-loop processing—for integration with an existing Brian model (see Figure 4.1). Cleo’s functionality centers around a CLSimulator object that orchestrates the interactions between these three modules and the Brian simulation by injecting devices, running the simulation, and communicating with an IOProcessor object at each time step. The IOProcessor receives recorder measurements according to a specified sampling rate and returns any updates to stimulator devices. To simulate the effects of latency in closed-loop control, Cleo provides a LatencyIOProcessor class which stores control signals in a buffer to later deliver them after an arbitrary delay. \n\n\n\nElectrode recording\n\nBecause we have prioritized point neuron simulations, the electrode functionality currently implemented in Cleo does not rely on raw extracellular potentials, which can only be computed from multi-compartment neurons (5, 6). This biophysical forward modeling approach has been taken in other software (7–10). To approximate sorted and multi-spike recording without filtering and thresholding of extracellular potentials, Cleo simply takes ground-truth spikes and stochastically determines which to report with probability inversely proportional to distance to the electrode (11, 12) (see Figure 4.2).\n\n\n\n\n\n\n\nFigure 4.2: Illustration of LFP and spiking from Cleo’s electrophysiology module. (A) The probabilistic spike detection model. All spikes within the 100% detection radius, 50% of spikes at the 50% detection radius, and none of those outside the threshold radius are recorded. The detection probability decays with \\(1/r\\). (B) An example plot generated by Cleo showing the positions of neurons and electrode contacts. (C) Randomly generated spikes for the neurons shown in B. Top: the sorted spike signal, which gives the ground truth source neuron for every spike as a perfect proxy for spike sorting. Bottom: multi-unit spikes, where spikes are reported on every channel they are detected on, regardless of the source neuron. (D) The TKLFP signal generated from the spikes in C for each channel. Y-axis units are not shown.\n\n\n\nIn order to approximate cortical LFP without recurring to morphological neurons and biophysical forward modeling, we implemented the kernel LFP approximation from (13), which we term TKLFP (Teleńczuk kernel LFP). This method approximates the per-spike contribution to LFP (termed uLFP: unitary LFP) with a delayed Gaussian function, where amplitude and delay depend on the position of the neuron relative to the electrode.  This implementation is available as a standalone Python package on PyPI (14). Accuracy of this implementation is verified in automated test suites in both TKLFP and Cleo packages.\n\n\nOptogenetic stimulation\n\nCleo simulates optogenetic stimulation by combining a model of light propagation with an opsin model relating light to current. The light model is based on Kubelka-Munk light propagation, operating on the assumption that the medium is optically homogeneous and that particles are larger than the light wavelength (15, 16). Cleo includes absorbance, scattering, and refraction parameters for 473-nm (blue) light as given in (15).\n\nIndependent of the light propagation model, Cleo provides two different opsin models. One is a four-state Markov model as presented in (17), which captures rise, peak, plateau, and fall dynamics as the opsin is activated and deactivated through a Markov process. Additionally, by defining conductance rather than current directly, the model is able to reproduce the photocurrent’s dependence on the membrane potential (see Figure 4.3).  Because the Markov model depends on somewhat realistic membrane potential and resistance values, however, it is not well suited for many pre-existing models that do not. To that end, Cleo also provides a simplified model that delivers current (of arbitrary units) proportional to light at the neuron. Users can also specify both the probability that cells of a target population express an opsin and the per-cell expression level."
  },
  {
    "objectID": "src/aim1.html#results",
    "href": "src/aim1.html#results",
    "title": "4  Aim 1 - A CLOC simulation testbed",
    "section": "Results",
    "text": "Results\n\nModel validation\nTo verify that light and opsin models behaved as expected, we reproduced figures illustrating the optic fiber light transmission profile found in Figure 2a,b of (15) and the photocurrent in response to the ramping light intensity from Figure 4c of (17). Light intensity-firing rate relationship was qualitatively similar to that originally reported, though differing in some respects due to the use of point neurons rather than morphological neurons. See Figure 4.3 for details. Additionally, preliminary experiments show that the simplified, proportional current opsin model is able to produce a firing response qualitatively similar to that of the Markov model.\n\n\n\n\n\n\n\nFigure 4.3: Validation of the optogenetics module. (A) Light transmittance \\(T\\) as a function of radius and axial distance from the optic fiber tip. Transmittance refers to the irradiance \\(\\text{Irr}\\) as a proportion of the irradiance at the fiber tip \\(\\text{Irr}_0\\). After Figure 2a from (15). (B) Light transmittance \\(T\\) as a function of distance \\(z\\) straight out from the fiber tip for different optic fiber sizes. After Figure 2b from (15). (C) Photocurrent \\(I_\\text{opto}\\) for ramping light of different intensities. After the figure produced by the “ramp” protocol from the default PyRhO simulator (17). (D) Neuron firing rates in response to optical stimulation with 5-ms pulse frequencies ranging from 1 to 200 Hz. The left column re-plots data from (15). The middle column shows results for an LIF neuron with a simple opsin, and the right column for a tonic AdEx neuron (18) with a Markov opsin model. The top row shows results for different light intensities: 100%, 120%, and 140% of the threshold for producing a single spike with a 5-ms pulse. The bottom row shows results for different expression levels relative to the default, \\(\\rho_{\\text{rel}}\\). The irradiance used for these simulations was 120% of the single-spike threshold.\n\n\n\n\nExample experiments\nIn order to demonstrate Cleo’s utility to the public, we implemented three example experiments to feature in the upcoming publication:\n\nClosed-loop inhibition of a traveling wave in a rodent somatosensory cortex model (19). This experiment features a simple on/off control scheme where light activating an inhibitory opsin is turned on whenever the number of spikes detected at the optrode exceeds a threshold. The result is that while a wave of activity spreads outward through sensory cortex in response to a stimulus, the area around the optrode remains unperturbed. An experiment of this sort could explore the consequences of traveling waves in sensory cortex on perception.\nFeedback control of layer 2/3 interneurons, disrupting plasticity in a model of primary visual cortex (20). In this model, a top-down reward signal to vasoactive intestinal peptide (VIP) interneurons inhibits a population of parvalbumin (PV) interneurons, in turn disinhibiting a population of pyramidal cells (PC). However, using a proportional-integral (PI) feedback controller, we clamp the firing rate of interneurons, blocking this disinhibtion. Without increased PC firing, synaptic weights corresponding to the rewarded stimulus no longer increase, thus demonstrating how feedback control can disrupt normal circuit function.\nOptogenetic evocation of sharp wave-ripples in an anatomically detailed model of hippocampus (21, 22). This uses linear quadratic regulator control to evoke a reference sharp wave-ripple LFP signature. See Figure 4.4 for details, and note that feedback control, not looking ahead, so to speak, fails to evoke the reference signal at the desired time. The strategy I propose in Aim 2 should remedy this.\n\n\n\n\n\n\n\n\nFigure 4.4: An example application of Cleo to the anatomical hippocampus model from Aussel et al. (21, 22). (A) A 2.5 mm-thick slice of the 15 mm-tall model is shown. The model consists of four regions, entorhinal cortex (EC), dentate gyrus (DG), CA3, and CA1. Electrode contacts are represented as black dots and are in the same location as in the original model. Two light sources are shown in EC. Nine other such pairs (for a total of 20 light sources) not pictured here were spaced regularly parallel to the z axis. (B) Results are shown for ten trials each of three stimulation paradigms: naïve open-loop, where the input is simply a mirrored, rectified version of the reference signal; model-based open-loop, where a controller is run offline on a simulated model; and feedback control, where a controller is run using online measurements. Input \\(\\text{Irr}_0\\) is the light intensity at the tip of each optic fiber. The system output TKLFP refers to the Teleńczuk kernel LFP approximation.\n\n\n\nAdditionally, we have endeavored to make Cleo accessible to the public to aid in the exploration and adoption of CLOC methods. Cleo is open-source and can be installed from the Python Package Index under the name cleosim. The code can be found on GitHub. Documentation, including an overview, tutorials, and API reference, can be found at https://cleosim.readthedocs.io."
  },
  {
    "objectID": "src/aim1.html#limitations",
    "href": "src/aim1.html#limitations",
    "title": "4  Aim 1 - A CLOC simulation testbed",
    "section": "Limitations",
    "text": "Limitations\n\nBecause we (and Brian) prioritize point neuron models, Cleo does not currently support advanced, morphology-dependent features of neural dynamics as well as recording and stimulation. This would preclude realistically simulating extracellular potentials as well as the effects of spike sorting, for example.  However, if features such as these are needed, Cleo could be developed further to integrate with Brian’s morphological neuron features. And in the case of LFP, at least one other method exists for point-neuron LFP approximation which could be implemented to help compensate for the limitations of TKLFP (23).\nBoth a strength and a limitation of Cleo’s design is that it depends on whatever model the user provides. This avoids the pitfall of painstakingly developing stock models that may or may not prove useful to researchers and instead lets them identify or develop a model that adequately describes the phenomena being studied. If a sufficiently realistic model for the studied system does not exist, however, developing one may be prohibitively costly, becoming a computational project in its own right as opposed to simply a stepping-stone towards an experiment. In these cases we suggest that a workaround could be to test a variety of potential models to identify which experimental configurations would be robust to unknown properties of the system. Indeed, the desired experiment in this case could be one that best adjudicates between these hypotheses."
  },
  {
    "objectID": "src/aim1.html#summary-of-contributions",
    "href": "src/aim1.html#summary-of-contributions",
    "title": "4  Aim 1 - A CLOC simulation testbed",
    "section": "Summary of contributions",
    "text": "Summary of contributions\nCleo thus provides a simulation environment facilitating and accelerating the adoption and development of CLOC methods. Multiple features were designed to reflect this need, for example, neural simulation and experimental configuration at an intermediate level of realism, relevant recording and stimulation modalities, modular software architecture, flexible interfacing that can be easily adapted to existing Brian models, and arbitrary closed-loop processing enabling realistic processing delays. Moreover, we have developed virtual experiments to serve as examples as well as thorough online documentation so Cleo can help experimental labs test and potentially adopt CLOC methods.\n\n\n\n\n1. M. Stimberg, R. Brette, D. F. M. Goodman, Brian 2, an intuitive and efficient neural simulator. eLife. 8 (2019), doi:10.7554/eLife.47314.\n\n\n2. A. Bogdanchikov, M. Zhaparov, R. Suliyev, \"Python to learn programming\" in Journal of Physics: Conference Series (IOP Publishing, 2013), vol. 423, p. 012027.\n\n\n3. A. P. Davison, M. L. Hines, E. Muller, Trends in programming languages for neuroscience simulations. Frontiers in Neuroscience. 3, 374–380 (2009).\n\n\n4. E. Muller, J. A. Bednar, M. Diesmann, M. O. Gewaltig, M. Hines, A. P. Davison, Python in neuroscience. Frontiers in Neuroinformatics. 9, 11 (2015).\n\n\n5. K. H. Pettersen, H. Lindén, A. M. Dale, G. T. Einevoll, Extracellular spikes and CSD. Handbook of neural activity measurement. 1, 92–135 (2012).\n\n\n6. G. Buzsáki, C. A. Anastassiou, C. Koch, The origin of extracellular fields and currents EEG, ECoG, LFP and spikes. Nature Reviews Neuroscience 2012 13:6. 13, 407–420 (2012).\n\n\n7. E. Hagen, S. Næss, T. V. Ness, G. T. Einevoll, Multimodal modeling of neural network activity: Computing LFP, ECoG, EEG, and MEG signals with LFPy 2.0. Frontiers in Neuroinformatics. 12, 92 (2018).\n\n\n8. H. Parasuram, B. Nair, E. D’Angelo, M. Hines, G. Naldi, S. Diwakar, Computational modeling of single neuron extracellular electric potentials and network local field potentials using LFPsim. Frontiers in Computational Neuroscience. 10, 65 (2016).\n\n\n9. R. J. Tomsett, M. Ainsworth, A. Thiele, M. Sanayei, X. Chen, M. A. Gieselmann, M. A. Whittington, M. O. Cunningham, M. Kaiser, Virtual Electrode Recording Tool for EXtracellular potentials (VERTEX): Comparing multi-electrode recordings from simulated and biological mammalian cortical tissue. Brain Structure and Function. 220, 2333–2353 (2015).\n\n\n10. C. Thornton, F. Hutchings, M. Kaiser, The virtual electrode recording tool for extracellular potentials (VERTEX) Version 2.0: Modelling in vitro electrical stimulation of brain tissue. Wellcome Open Research. 4 (2019), doi:10.12688/wellcomeopenres.15058.1.\n\n\n11. S. R. Nason, A. K. Vaskov, M. S. Willsey, E. J. Welle, H. An, P. P. Vu, A. J. Bullard, C. S. Nu, J. C. Kao, K. V. Shenoy, T. Jang, H.-S. Kim, D. Blaauw, P. G. Patil, C. A. Chestek, A low-power band of neuronal spiking activity dominated by local single units improves the performance of brainmachine interfaces. Nature Biomedical Engineering 2020, 1–11 (2020).\n\n\n12. G. R. Holt, C. Koch, Electrical interactions via the extracellular potential near cell bodies. Journal of Computational Neuroscience. 6, 169–184 (1999).\n\n\n13. B. Telenczuk, M. Telenczuk, A. Destexhe, A kernel-based method to calculate local field potentials from networks of spiking neurons. Journal of Neuroscience Methods. 344, 108871 (2020).\n\n\n14. K. Johnsen, Kjohnsen/tklfp: V0.2.0 (2022), doi:10.5281/zenodo.6787979.\n\n\n15. T. J. Foutz, R. L. Arlow, C. C. Mcintyre, Theoretical principles underlying optical stimulation of a channelrhodopsin-2 positive pyramidal neuron. J Neurophysiol. 107, 3235–3245 (2012).\n\n\n16. T. Vo-Dinh, Biomedical Photonics: Handbook (CRC Press, 2003).\n\n\n17. B. D. Evans, S. Jarvis, S. R. Schultz, K. Nikolic, PyRhO: A Multiscale Optogenetics Simulation Platform. Frontiers in Neuroinformatics. 10, 8 (2016).\n\n\n18. W. Gerstner, W. M. Kistler, R. Naud, L. Paninski, Neuronal dynamics: From single neurons to networks and models of cognition (Cambridge University Press, 2014).\n\n\n19. S. Moldakarimov, M. Bazhenov, D. E. Feldman, T. J. Sejnowski, Structured networks support sparse traveling waves in rodent somatosensory cortex. Proceedings of the National Academy of Sciences of the United States of America. 115, 5277–5282 (2018).\n\n\n20. K. A. Wilmes, C. Clopath, Inhibitory microcircuits for top-down plasticity of sensory representations. Nature Communications. 10, 5055 (2019).\n\n\n21. A. Aussel, L. Buhry, L. Tyvaert, R. Ranta, A detailed anatomical and mathematical model of the hippocampal formation for the generation of sharp-wave ripples and theta-nested gamma oscillations. Journal of Computational Neuroscience. 45, 207–221 (2018).\n\n\n22. A. Aussel, R. Ranta, O. Aron, S. Colnat-Coulbois, L. Maillard, L. Buhry, Cell to network computational model of the epileptic human hippocampus suggests specific roles of network and channel dysfunctions in the ictal and interictal oscillations. Journal of Computational Neuroscience (2022), doi:10.1007/s10827-022-00829-5.\n\n\n23. A. Mazzoni, H. Lindén, H. Cuntz, A. Lansner, S. Panzeri, G. T. Einevoll, Computing the Local Field Potential (LFP) from Integrate-and-Fire Network Models. PLOS Computational Biology. 11, e1004584 (2015)."
  },
  {
    "objectID": "src/aim2.html#rationale",
    "href": "src/aim2.html#rationale",
    "title": "5  Aim 2 - Multi-input CLOC",
    "section": "Rationale",
    "text": "Rationale\n\nThe power of closed-loop optogenetic control (CLOC, henceforth referring specifically to feedback control; see Section 3.1) is limited by the degrees of freedom provided by the optogenetic stimulation. Naturally, we would want at least as many actuators as degrees of freedom in the system to control it effectively—for example, we may want to stimulate different layers, cell types, or columns separately in the cortex. Moreover, actuation/stimulation can be unidirectional or bidirectional, referring to whether a single opsin type or both excitatory and inhibitory opsins are used simultaneously. Unidirectional control has obvious shortcomings: for example, an excitatory opsin alone could not lower the firing rate of transfected cells, making it unsuitable to clamp baseline activity or follow a time-varying reference with steep drops.\n\nWhile a previous study (1) has implemented bidirectional CLOC, it does not feature the generalizability and scalability of the model-based, optimal control algorithms introduced by later work (2) (see Section 3.3) for unidirectional actuation. This adaptive linear-quadratic regulator (LQR) approach is more robust to disturbances and can scale to multi-input multi-output (MIMO) systems. Moreover, its behavior can be easily configured by setting penalties on state error, the control signal, and even the derivative of the control signal to encourage smooth actuation.\n\nThus, a natural goal for furthering CLOC is to combine the advantages of multi-input/bidirectional actuation and model-based optimal control—however, this poses additional challenges and opportunities. The adaptive LQR method previously developed has limited application for multi-input actuation because it does not model the constraint that the input (light intensity) must be nonnegative.  This would cause problems in the case of bidirectional (or any spatially overlapping) actuation, since the controller might call for negative excitatory input rather than positive excitatory input. A heuristic workaround to this is to place light sources in spatially non-overlapping pairs (e.g., blue and red) and treat them as the positive and negative directions of a single actuator. This allows the continued use of simple and fast LQR methods but fails to model kinetic differences or spectral crosstalk between inhibitory and excitatory opsins and precludes alternate light configurations. While I hypothesize that methods for optimal control with constraints will outperform this heurstic LQR, such methods are more costly computationally. It is thus unclear which method(s) are preferable for real-time control on the timescale of network-level variables of neural activity using compute resources typically available to an experimental neuroscience lab.\n\nInnovation\nI propose addressing this problem by comparing LQR to model predictive control (MPC), which is widely used for its flexibility in implementing optimal control with constraints. Rather than computing the control signal from the current error signal at each step, MPC looks ahead, optimizing over the predicted trajectory some finite number of steps into the future, in what is known as “receding horizon control.” The quadratic program optimization required at every control step, however, introduces latency which could harm control performance compared to LQR. I thus plan on testing my hypothesis that MPC will be able to better optimize multi-input optogenetic stimulation while accommodating experimental constraints and considerations during real-time control on timescales relevant to network-level descriptions of neural activity. I will do this by assessing control quality of MPC compared to the heuristic LQR approach previously described as I simulate multi-output feedback control of firing rates and gamma oscillations observed in local field potentials (LFP).\n\n\n\n\n\n\n\n\n\nFigure 5.1: An illustration of how MPC optimizes the system input over a receding horizon. By Martin Behrendt, licensed under CC BY-SA 3.0."
  },
  {
    "objectID": "src/aim2.html#approach",
    "href": "src/aim2.html#approach",
    "title": "5  Aim 2 - Multi-input CLOC",
    "section": "Approach",
    "text": "Approach\n\nSystem and controller formulation\nNaturally, the model is a vital element of MPC. I will use a previously developed Gaussian linear dynamical system (GLDS) model (2), which has been shown to reliably capture firing rate dynamics in a light-driven spiking neuron system. The discrete-time GLDS is governed by the following equations:\n\\[\n\\begin{aligned}\nx_{t + 1} &= Ax_{t} + Bu_{t} + w_t  \\\\\ny_{t} &= Cx_{t} + d  \\\\\nz_{t} &= y_{t} + v_t  \\\\\n\\end{aligned}\n\\]\nwhere \\(x_{t} \\in \\mathbb{R}^n\\) is the \\(n\\)-dimensional state, \\(u_{t} \\in \\mathbb{R}^k\\) is the \\(k\\)-dimensional stimulus (i.e., \\(k = 2\\) for two opsins, one light source each), \\(y_{t} \\in \\mathbb{R}^m\\) is the firing rate in spikes/timestep (for each of \\(m\\) measured neurons), and \\(z_{t} \\in \\mathbb{R}^m\\) is the number of binned spikes observed at time \\(t\\). \\(A \\in \\mathbb{R}^{n \\times n}\\), \\(B \\in \\mathbb{R}^{n \\times k}\\), and \\(C \\in \\mathbb{R}^{m \\times n}\\) are the state transition, input, and output matrices, respectively. \\(w_{t} \\sim \\mathcal{N}\\left( 0, Q \\right)\\) and \\(v_{t}\\mathcal{\\sim N}\\left( 0, R \\right)\\) are Gaussian-distributed process and measurement noise, respectively, and \\(d \\in \\mathbb{R}^{m \\times 1}\\) represents baseline firing rates. Model order (\\(n\\)) and horizon length (\\(T \\in \\mathbb{N}\\)) will be chosen to balance complexity and prediction error for noise-driven fitting data generated from the test network. The latent state \\(x_{t}\\) will be estimated online using the Kalman filter (3), driven by the prediction error \\(z_{t} - {\\widehat{y}}_{t|t - 1}\\).\nI will set hard non-negative constraints on the light input as well as a ceiling determined by hardware limitations (i.e., the maximum voltage deliverable to the LED driver). To design an appropriate cost function, I will use a conventional per-time step quadratic form\n\\[\\ell( x_{t},r_{t},u_{t} ) = ( x_{t} - r_{t} )^{T}Q^{\\text{ctrl}}( x_{t} - r_{t} ) + u^{T}R^{\\text{ctrl}}u\\ ,\\]\nwhere \\(r_{t} \\in \\mathbb{R}^n\\) is the reference trajectory at time \\(t\\). \\(Q^{\\text{ctrl}}\\) and \\(R^{\\text{ctrl}}\\) are real \\(n \\times n\\) and \\(k \\times k\\) matrices chosen to appropriately penalize tracking error and stimulus size, respectively. This quadratic cost function formulation lends the problem well to standard optimization techniques—combined with a linear dynamical system, it constitutes the classical linear-quadratic-Gaussian (LQG) control problem.\nThen, at every time step \\(t\\) the controller solves the following quadratic program:\n\\[\n\\begin{aligned}\n    \\text{minimize}{} \\quad & \\sum_{\\tau=t}^{t+T} \\ell(x_\\tau, u_\\tau) \\\\\n    \\text{subject to} \\quad & u_\\tau \\succeq 0 \\\\\n        & x_{\\tau + 1} = Ax_{\\tau}+Bu_\\tau \\\\\n\\end{aligned}\n\\]\nwhere \\(T \\in \\mathbb{N}\\) is the number of steps in the prediction/control horizon and \\(\\succeq\\) indicates an inequality for each element of \\(u_\\tau \\in \\mathbb{R}^k\\). This yields the solution \\(\\tilde{u}_\\tau,...,\\tilde{u}_{\\tau+T-1}\\), of which we take just the first step \\(\\tilde{u}_t\\) to apply to the system.\n\n\nControl method comparison in silico\nTo confirm our assumption that bidirectional and multi-channel configurations will improve control quality, I will test both unidirectional and bidirectional actuation as well as one-channel and multi-channel configurations for each experiment. (In the bidirectional actuation case, one “channel” includes both an inhibition-triggering and an excitation-triggering light source). Another condition of interest will be to penalize a low-pass-filtered version of the stimulus, which could reflect overheating or ion imbalances caused by prolonged stimulation (4–6). This could be added to the linear dynamics and quadratic cost functions without changing the optimization methods, but, as with opsin and channel count, the increased size of the problem increases latency which could affect which method performs best. Most importanly, I will compare open-loop, heuristic LQR, and MPC approaches to see whether MPC attains better performance despite a longer computational delay, as hypothesized. Control algorithm computation time will be measured for each method and used during simulations as a realistic delay. To evaluate controller performance, I will use metrics such as the mean-squared error (MSE).\nIn the first experiment, I will simulate multi-output control of firing rates in an attempt to clamp population activity. (7) and (8) laid the foundation for this by clamping aggregate, population firing rates, and (2) took this further by treating firing rates of individual neurons separately, though with a single optic fiber input. This is an obvious case where multiple inputs should provide more control, allowing us to manipulate neurons (or groups of neurons in the case of unsorted threshold crossings) more individually, each to its own baseline. The experiment will be performed on a Poisson linear dynamical system (PLDS) model (9) fit to optogenetic input/spike output data from a spiking neural network (SNN) model. The SNN model will contain necessary features, such as cell types, time-varying exogenous input, and connectivity profiles, to produce stochastic firing patterns with unpredictable disturbances and will be simulated together with the recording and stimulation facilities of Aim 1. \nAs another experiment, I propose another form of network-level control—manipulating the oscillatory signatures found in LFP signals. Gamma oscillations (30-90 Hz) are one such example, which have been shown to have time-varying properties and interact with other oscillatory bands. Interestingly, gamma rhythms can even show phase coherence in distant brain regions such as visual cortex in opposite hemispheres, which has been hypothesized to play a role in information integration (10). I plan to test control methods on a virtual experiment of this phenomenon by simulating two populations of neurons exhibiting gamma oscillations (11, 12) with long-range connections mediating gamma frequency coherence, again using the framework from Aim 1. I will then perform feedback control on the LFP signal to counter the natural phase locking that arises, providing an opportunity to test how well LFP can be controlled on a <100-ms timescale of a single gamma cycle."
  },
  {
    "objectID": "src/aim2.html#expected-results",
    "href": "src/aim2.html#expected-results",
    "title": "5  Aim 2 - Multi-input CLOC",
    "section": "Expected results",
    "text": "Expected results\nI expect that bidirectional, multi-channel, and model predictive control will be perform better than unidirectional, single-channel, and LQR control, respectively for both experiments, despite the added computational cost these methods require. I also expect that restricting the controller through hard constraints or prolonged stimulation penalties will also be possible with the more sophisticated methods without greatly increasing the error."
  },
  {
    "objectID": "src/aim2.html#preliminary-results",
    "href": "src/aim2.html#preliminary-results",
    "title": "5  Aim 2 - Multi-input CLOC",
    "section": "Preliminary results",
    "text": "Preliminary results\nBasic simulations controlling a linear dynamical system model fit to experimental data show the advantages of bidirectional control and of MPC (see Figure 5.2). Bidirectional actuation allows the system to avoid overshooting the reference, in the case of LQR, or to minimize error faster by first exciting then inhibiting, in the case of MPC. MPC’s advantages in looking ahead also clearly allow it to follow the reference more closely than the heuristic LQR controller (assigning negative inputs to the second light source). However, these results do not yet account for control signal computation latency.\n\n\n\n\n\nFigure 5.2: Simulated control of an linear dynamical system with 1- and 2-input control, using LQR and MPC controllers. The top panel of each contains the reference and the actual firing rate, in spikes/second. The bottom contains the light intensity, in terms of mW/mm2, where blue represents light for an excitatory opsin (such as ChR2) and red-orange that for an inhibitory opsin (such as Jaws)."
  },
  {
    "objectID": "src/aim2.html#sec-aim2-pitfalls",
    "href": "src/aim2.html#sec-aim2-pitfalls",
    "title": "5  Aim 2 - Multi-input CLOC",
    "section": "Potential pitfalls & alternative strategies",
    "text": "Potential pitfalls & alternative strategies\nThere are some limitations in the proposed GLDS model that may need to be addressed. While it was adequate for the experiments in (2), a Poisson (9) or other output nonlinearity may be needed in the case that a standard GLDS does not fit the data well. While this would likely make the estimation of \\(x\\) more expensive, the underlying dynamics could remain linear, leaving the same underlying quadratic program for the controller to solve.\nThis touches another potential concern: the speed of MPC. If the optimization problem solution is slow, there are a few options to explore. One is that some variations in the control scheme can help balance speed and performance, such as letting the control horizon be shorter than the prediction horizon, which shrinks the optimization problem. Likewise, the control period can be longer than the time step of the system, reducing how often the control signal is computed. If conventional methods such as these are unsuccessful, I may turn to approximate methods such as that described in (13) or training an artificial neural network. Another potential solution is explicit MPC (14), which finds a piecewise-affine explicit solution to the quadratic program which can be faster than obtaining the implicit solution for small-enough problems.\n\n\n\n\n1. J. P. Newman, M. F. Fong, D. C. Millard, C. J. Whitmire, G. B. Stanley, S. M. Potter, Optogenetic feedback control of neural activity. eLife (2015), doi:10.7554/eLife.07192.\n\n\n2. M. F. Bolus, A. A. Willats, C. J. Rozell, G. B. Stanley, State-space optimal feedback control of optogenetically driven neural activity. Journal of neural engineering. 18, 036006 (2021).\n\n\n3. R. E. Kalman, A new approach to linear filtering and prediction problems. Journal of Fluids Engineering, Transactions of the ASME. 82, 35–45 (1960).\n\n\n4. O. Yizhar, L. E. Fenno, T. J. Davidson, M. Mogri, K. Deisseroth, Optogenetics in Neural Systems. Neuron. 71, 9–34 (2011).\n\n\n5. M. Kokaia, M. Andersson, M. Ledri, An optogenetic approach in epilepsy. Neuropharmacology. 69, 89–95 (2013).\n\n\n6. J. M. Stujenske, T. Spellman, J. A. Gordon, Modeling the Spatiotemporal Dynamics of Light and Heat Propagation for InVivo Optogenetics. Cell Reports. 12, 525–534 (2015).\n\n\n7. D. A. Wagenaar, R. Madhavan, J. Pine, S. M. Potter, Controlling bursting in cortical cultures with closed-loop multi-electrode stimulation. Journal of Neuroscience. 25, 680–688 (2005).\n\n\n8. J. Newman, R. Zeller-Townson, M. Fong, S. Arcot Desai, R. Gross, S. Potter, Closed-Loop, Multichannel Experimentation Using the Open-Source NeuroRighter Electrophysiology Platform. Frontiers in Neural Circuits. 6 (2013).\n\n\n9. J. H. Macke, L. Buesing, J. P. Cunningham, B. M. Yu, K. V. Shenoy, M. Sahani, \"Empirical models of spiking in neural populations\" in Advances in Neural Information Processing Systems (Curran Associates, Inc., 2011), vol. 24.\n\n\n10. G. Buzsáki, X.-J. Wang, Mechanisms of Gamma Oscillations. Annual Review of Neuroscience. 35, 203–225 (2012).\n\n\n11. X.-J. Wang, G. Buzsáki, Gamma oscillation by synaptic inhibition in a hippocampal interneuronal network model. Journal of neuroscience. 16, 6402–6413 (1996).\n\n\n12. N. Brunel, X.-J. Wang, What determines the frequency of fast network oscillations with irregular neural discharges? I. Synaptic dynamics and excitation-inhibition balance. Journal of neurophysiology. 90, 415–430 (2003).\n\n\n13. Y. Wang, S. Boyd, Fast Model Predictive Control Using Online Optimization. IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY. 18, 267 (2010).\n\n\n14. A. Bemporad, M. Morari, V. Dua, E. N. Pistikopoulos, The explicit linear quadratic regulator for constrained systems. Automatica. 38, 3–20 (2002)."
  },
  {
    "objectID": "src/aim3.html#rationale",
    "href": "src/aim3.html#rationale",
    "title": "6  Aim 3 - Control of latent population dynamics",
    "section": "Rationale",
    "text": "Rationale\n\nAs technology for recording from the brain has improved, systems neuroscience has shifted increasingly towards a reduced-dimensionality, population coding perspective in many brain areas (1–4). This reflects the observation that while the activity of any single neuron can vary greatly across trials where external variables are controlled, underlying latent variables can be decoded from the population which are much more predictable and reproducible. Moreover, formulating this latent variable as a dynamical state whose evolution can be predicted has been shown to improve inference and enable state-of-the-art “decoding” of downstream variables of interest such as movement (5–8), mood (9), speech (10), and decision states (11, 12). Not only does this allow us to infer the output of a given brain region, but it allows us to form hypotheses about how it produces that output by analyzing the dynamical landscape of fixed points (13–15)—a view formalized in the Computation through Dynamics (CTD) framework (16).\n\nHowever, while these latent variables have been used to successfully decode other variables of interest, this is a necessary, but insufficent, condition to demonstrate a causal relationship. That is, an association between neural activity variable \\(a\\) and some other variable \\(b\\) may reflect the causal relationship \\(a \\rightarrow b\\), but could also reflect \\(b \\rightarrow a\\) or \\(a \\leftarrow c \\rightarrow b\\). This may be adequate for brain-computer interface (BCI) applications, but verification of that causal relationship is necessary for neuroscience’s goal of deepening our understanding of the architecture and algorithms of brain computations. This requires experimental control (17) on the level of neural populations, but, as stated by (16),\n\nThe challenge is nontrivial; testing CTD models requires a high degree of control over neural activity. The experimenter must be able to manipulate neural states arbitrarily in state space and then measure behavioral and neural consequences of such perturbations[.]\n\n\nCLOC is a natural candidate for this kind of experimental control of latent states for various reasons. The optimal state-space methods already formulated map directly to the latent dynamics models which have generated so much interest in recent systems neuroscience. High-dimensional optogenetic actuation is possible through micro-LED devices (18–29), two-photon targeting of individual neurons (30–34), and genetic targeting. Moreover, real-time feedback can drive a variable neural system towards complex latent state targets which would be attainable with low accuracy at best and not at all at worst with open-loop stimulation 1. However, while it is clear a high degree of control will be needed, it is unknown how this translates to recording, stimulation, and control parameters. One significant open question is whether this kind of control will require actuation on the level of individual neurons, or whether lower-dimensional actuation will suffice. I hypothesize that the latter is the case, but testing this hypothesis will likely require extensive trial and error, as well actuation hardware that does not exist or is not readily available, making it costly if not infeasible to do in vivo.\n\nInnovation\nThus, to give a preliminary answer to this question, I propose to develop technical and conceptual guidelines as I control the latent dynamics of simulated neural populations. First, I will produce virtual models by training recurrent spiking neural networks with state-of-the-art, biologically plausible methods—each differing in their degrees of brain-like architecture and training procedure complexity. I will then use the simulation testbed of Aim 1 and the multi-input control methods of Aim 2 to explore how control quality varies with both experimental parameters (such as recording and stimulation channel counts or control algorithms) and system characteristics (such as the size, complexity of the network model). This will both give researchers a tentative idea of the relative importance of each factor of CLOC and, as parameters approach experimental realism, allow us to conjecture as to the number of independent actuators needed to control the latent dynamics of a brain system. My hypothesis is that control quality will plateau with a number of actuators on or near the same level of magnitude as the latent factors used to describe the neural activity—that per-neuron actuation limit not be needed."
  },
  {
    "objectID": "src/aim3.html#approach",
    "href": "src/aim3.html#approach",
    "title": "6  Aim 3 - Control of latent population dynamics",
    "section": "Approach",
    "text": "Approach\n\n\n\nFigure 6.1: Overview of latent factor control experiments.\n\n\n\n\n\n\n\n\nFormulate task with latent dynamics hypothesis\nTo model an experiment of interest to neuroscientists, I will first choose a task where latent dynamic analyses have yielded testable hypotheses in animal research. For example, it is hypothesized that motor cortex works as a dynamical system where movement planning essentially sets an initial condition during a preparatory phase. This is demonstrated specifically by decoding reach direction in a delayed reach task from dorsal premotor cortex before movement onset (35, reviewed in 36) (see Figure 6.2). One way to causally test this hypothesis would be to manipulate the latent factors corresponding to a certain reach direction despite an absent or contradictory cue and verify whether the subject reaches in the predicted direction.\n\n\n\nFigure 6.2: Illustration of a delayed center-out reach experiment. From (37).\n\n\n\n\n\nTrain RSNN models\nAfter identifying a task and latent dynamics hypothesis to test, I will train recurrent spiking neural network (RSNN) models to perform the task. These RSNN models will be defined in 3D space to make them compatible with the electrode and optogenetics models from Aim 1. To avoid painstaking implementation of every known anatomical detail of the brain region(s) involved in the task—an approach which is not guaranteed to include every important detail or capture true circuitry—I propose using a simple, abstract form such as a rectangular prism of cortical tissue and measuring the effect of adding progressivley more brain-like structural constraints (see Section 6.2.6).\nI will then train these models to perform the given task. While a few different methods for training RSNNs exist, I plan on using e-prop (38), a biologically plausible approximation to backpropagation through time (BPTT), the method typically used to train artificial recurrent neural networks (RNNs). This biological plausibility consists in updating synaptic weights using only local information about past activity—“eligibility traces”—and a top-down learning signal and enables learning sparsely firing spike-coding—as opposed to only rate-coding—solutions. While e-prop has been shown to learn complex tasks such as Atari games via reinforcement learning, I propose a simpler supervised learning approach. This is both more data-efficient and would directly allow for the learning of auxiliary variables, which has been shown in one study to make model dynamics more brain-like (39) (see Section 6.4).\nTo give a concrete example of inputs and outputs for the case of the delayed center-out reaching task before mentioned, inputs might include the task phase (stop or go), the target position, and the current hand position. The model output might be hand velocity, and the learning signal could be computed from the x/y distance between the current position and the center (target) before (after) the go cue is given. A regularizer term on acceleration could be added to ensure trajectories are smooth.\n\n\nFit dynamical systems model\nAfter a model has been trained to some threshold performance level, I will record spikes and/or LFP from the network while it performs the task. The resulting data will then be used to fit a low-dimensional dynamical systems model. Seeing that the goal of the virtual experiment is to test the causal effect of a latent factor on model output, I propose using system identification methods that prioritize the discovery of those latent factors that are most relevant to behavior. The preferential subspace identification (PSID) method presented by (7) is a good candidate, having been shown to predict behavior well with few dimensions. The linear system it produces is also ideal for the control theory methods I develop in Aim 2. In a follow-up study, (8) introduce RNN-PSID supporting nonlinearities as well and find empirically that linear dynamics with only a nonlinear mapping from latent to behavior are often sufficient to describe experimental data well. Using a potentially nonlinear output mapping in this way would increase the expressiveness of my models while maintaining underlying linear dynamics for fast optimization.\nAfter fitting a model to data, I will identify latent factors corresponding to variables of interest. These could be individual components of the latent state \\(x\\) directly or simple (e.g., linear) transformations of \\(x\\). In the delayed center-out reaching task, for example, I would expect two factors with which the state at the end of the preparatory period can be used to predict the cued target position.\n\n\nControl of latent factors\nThe next step is then to test whether these latent factors do in fact cause the behavior we observe. Importantly, the model and hypothesis from the previous step were formed without stimulation, since our stated goal is to causally test the latent factors we deduce from passive observation. Thus, I will first need to expand the model by simulating random optogenetic stimulation and fitting an input model—e.g., an input matrix \\(B\\) for the typical LDS case. If this is insufficient to fit the data well, I may need to increase the dimensionality of the latent state \\(x\\) to account for the actuator state (i.e., opsin kinetics) or dimensions of neural activity not arising during unperturbed activity.\nWith an input model, I will then be able to run control experiments. The methods from Aim 2 will calculate optimal actuation strategies as the simulation progresses to drive the identified latent factors to the desired state. Control quality will be assessed for each experiment using metrics such as mean-squared error (MSE) and will serve as the criterion for how successful the experiment was, seeing that low error serves the larger goal of testing the causal relationship with model behavior.\n\n\n\nExploration of experiment parameters and expected effects\nTo guide experimental design, I propose repeating the above process many times to explore how different factors affect control quality, thus informing which investments might be most fruitful. These factors might include:\n\nControl type. Open-loop control is faster and easier but cannot counteract variability inherent to the system and in the presence of model mismatch will fail to reach the target. Closed-loop methods will almost certainly perform better, but are harder to implement. Moreover, closed-loop control offers a range of options spanning the cost/quality spectrum, with the fast linear quadratic regulator (LQR) on one end and the expensive long-horizon, high-resolution model predictive control (MPC) on the other.\nTotal recording/stimulation channel count (\\(m + k\\)). It may be helpful to think of these two parameters together, since they both require limited space in a device that is placed in or on the brain. I expect control quality to increase with channel count up to a point where the computational cost required for closed-loop control outweighs the benefits.\nRecording/stimulation channel ratio (\\(m/k\\)). Related to the previous point, when space for interface is hardware, it is an open question whether the current landscape of neuroscience technology, which has prioritized high-channel-count recording over stimulation, is the most effective for causal perturbations of neural dynamics.\nData collection budget. Data collection is not free, and thus must be considered especially when considering the data-efficiency advantages of feedback control. This is also relevant for choosing models to control the system—with little training data, for example, a simple, easy-to-fit model may outperform a more expressive, data-hungry one.\nOptogenetic stimulation parameters. These could include the number and type of opsins and genetic targeting. Using at least two opsins could be expected to perform better than just one, as shown in Aim 2, though many more might not be useful due to overlapping spectral sensitivity and increased computation time. The effect of genetic targeting will likely depend on how well the targeted population correlates with the latent factor of interest.\n\n\n\nExploration of model realism and expected effects\nAs explained above, I propose using abstract RSNN models with varying degrees of brain-like realism rather than extremely detailed models. This is because highly detailed models are difficult to develop and expensive to simulate—moreover, it is hard to say exactly when a model has enough to detail to behave similarly to a real brain. On the other hand, by assessing the effect of a handful of brain-like features on how well we can contol latent neural dynamics in silico, I hope to identify trends that give a rough outlook of doing so in vivo. For example, if control quality tends to drop with the addition of each feature, we might infer control of a real brain will be considerably more challenging than in simulations. On the other hand, if control quality does not decrease considerably with brain similarity, we might have reason to be more optimistic.\nThe brain-like features I propose assessing include structural characteristics such as:\n\nVariability. Brains are far from deterministic, with unpredictability on the level of synaptic transmission all the way up to firing rates across a population. Small-scale stochasticity encourages redundance and robustness, which may make the model harder to “hack,” or perturb unnaturally. I expect that large-scale variability, such as an unmodeled input to the region, will highlight the advantages of feedback control.\nNetwork size (how many neurons are in the model). More neurons would likely mean more dimensions along which activity can vary, complicating the prospect of arbitrary control.\nSubpopulation structure. This could include constraining connectivity to circuits of brain regions, cortical layers, and cell types. This could also make control harder if the latent factor to control operates in cells that are multiple synapses away from those stimulated or if dynamics become driven by recurrent connections more than stimulation.\nConnectivity profiles. Cell pairs within a given population might have a uniform or a spatially dependent connection probability. I expect this, like subpopulation structure, to encourage segregated dynamics which could be either easier or harder to perturb.\nCell diversity. Cells might have uniform or a random distribution of parameters such as membrane resistance, synaptic transmission delays, resting potentials, adaptivity, etc. Again, I expect this would encourage the segregation of neural dynamics.\nNeuron model complexity. Individual cell models could range from simple leaky integrate-and-fire (LIF) to Hodgkin-Huxley dynamics. I do not expect this to considerably impact control quality.\n\nThey also include characteristics about how the model is trained. A real brain regions is capable of performing not only one, simple, stereotyped task, but many tasks in many contexts. Therefore, another way to make models more brain-like is to diversify their experience—to train them on other tasks. It has been shown that this can result in an RNN reusing computational motifs learned in one task to solve others (40, 41)."
  },
  {
    "objectID": "src/aim3.html#expected-results",
    "href": "src/aim3.html#expected-results",
    "title": "6  Aim 3 - Control of latent population dynamics",
    "section": "Expected results",
    "text": "Expected results\nIn addition to the specific expectations for experimental and model parameters described above, I expect that control quality will be relatively low (error will be high) especially for low actuator counts \\(k\\). However, as previously stated, I do not anticipate that \\(k\\) will need to approach the per-neuron limit either; rather, that low-error control will be possible with \\(k\\) close to the same order of magnitude as the latent state dimensionality. While expectations for each brain-like model feature are explained above, I predict that on the whole, control quality will decrease as models become more realistic. However, I predict that that that decrease will plateau, providing some confidence in my results and assurance that the complexity of the real brain may not be an insurmountable barrier to future in-vivo experiments."
  },
  {
    "objectID": "src/aim3.html#sec-aim3-pitfalls",
    "href": "src/aim3.html#sec-aim3-pitfalls",
    "title": "6  Aim 3 - Control of latent population dynamics",
    "section": "Potential pitfalls & alternative strategies",
    "text": "Potential pitfalls & alternative strategies\nOne potential problem is that the RSNN models could learn dynamics that are not very brain-like. For example, they might learn a complicated feedforward function instead of tracking latent variables in a more natural way, in which case explicitly teaching the models to track variables of interest has been shown to make dynamics more brain-like (39). Alternatively, the latent variables decoded could be dominated by very few neurons, thus producing “latent variables” which are not actually latent and distributed, as in the brain. This can be avoided by regularizing the state inference step to discourage sparsity, thus inferring the state from a more diverse set of neurons.\nAnother caveat to generalizing conclusions is that the proposed perturbations may be unnatural—by perturbing the latent factors decoded from passive observation, I may unwittingly by pushing neurons “off-manifold,” causing them to fire in unnatural combinations (42). While it is beyond the scope of the proposed work to avoid this, I can at least quantify the phenomenon via the proxy of neural activity unexplained by our latent dynamical, assuming our intrinsic manifold is a subspace rather than a lower-dimensional attractor structure embedded therein (16, 43). For example, if under passive observation we can predict 95% of neural activity, and that drops to 50% under perturbation, we could conclude that 45% of the activity is now unnatural, as it must be explained by dimensions of activity introduced by stimulation.\nBesides conceptual obstacles, the proposed work also has practical challenges, such as its large scale. To adequately assess control prospects across the whole space of experiment and structural factors described could represent a prohibitively high computational cost. This can be mitigated by avoiding a grid search of this space, analyzing random combinations of features or just one at a time rather than every single combination. Another factor in the computational cost is the speed of the training, model fitting, and control simulations themselves. In the case that simulations are too slow with the Brian/Cleo framework described in Aim 1, it may be worth exploring simpler simulations for the training step which does not require Cleo. This could involve using fast differential equation solvers (44) for the training step or even a less biological learning algorithm such as backpropagation through time with surrogate gradients for the nondifferentiable spike threshold function (45). Training might also be accelerated using a form of “warm start” where weights are initialized from a pre-trained model, though care would need to be taken that the effects of the warm start on learned dynamics not overshadow those of the structural features being tested.\nAdditionally, there is a possibility that RNN-PSID requires nonlinear terms to fit model data well, despite the authors’ findings that linear dynamics are often sufficient (8). In the case that nonlinearities preclude the formulation of MPC as a fast quadratic program, I may need to turn to neural networks, which, after training, could quickly produce approximate solutions for nonlinear optimization problems (see Section 5.5).\n\n\n\n\n1. J. F. Kalaska, R. Caminiti, A. P. Georgopoulos, Cortical mechanisms related to the direction of two-dimensional arm movements: Relations in parietal area 5 and comparison with motor cortex. Experimental Brain Research. 51, 247–260 (1983).\n\n\n2. M. M. Churchland, J. P. Cunningham, M. T. Kaufman, J. D. Foster, P. Nuyujukian, S. I. Ryu, K. V. Shenoy, K. V. Shenoy, Neural population dynamics during reaching. Nature. 487, 51–56 (2012).\n\n\n3. J. P. Cunningham, B. M. Yu, Dimensionality reduction for large-scale neural recordings. Nature neuroscience. 17, 1500–9 (2014).\n\n\n4. D. L. Barack, J. W. Krakauer, Two views on the cognitive brain. Nature Reviews Neuroscience. 22, 359–371 (2021).\n\n\n5. C. Pandarinath, D. J. O’Shea, J. Collins, R. Jozefowicz, S. D. Stavisky, J. C. Kao, E. M. Trautmann, M. T. Kaufman, S. I. Ryu, L. R. Hochberg, J. M. Henderson, K. V. Shenoy, L. F. Abbott, D. Sussillo, Inferring single-trial neural population dynamics using sequential auto-encoders. Nature Methods. 15, 805–815 (2018).\n\n\n6. F. R. Willett, D. T. Avansino, L. R. Hochberg, J. M. Henderson, K. V. Shenoy, High-performance brain-to-text communication via handwriting. Nature. 593, 249–254 (2021).\n\n\n7. O. G. Sani, H. Abbaspourazad, Y. T. Wong, B. Pesaran, M. M. Shanechi, Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification. Nature Neuroscience. 24, 140–149 (2021).\n\n\n8. O. G. Sani, B. Pesaran, M. M. Shanechi, M. Hsieh, Where is all the nonlinearity: Flexible nonlinear modeling of behaviorally relevant neural dynamics using recurrent neural networks. bioRxiv, 2021.09.03.458628 (2021).\n\n\n9. O. G. Sani, Y. Yang, M. B. Lee, H. E. Dawes, E. F. Chang, M. M. Shanechi, Mood variations decoded from multi-site intracranial human brain activity. Nature Biotechnology. 36, 954–961 (2018).\n\n\n10. G. K. Anumanchipalli, J. Chartier, E. F. Chang, Speech synthesis from neural decoding of spoken sentences. Nature. 568, 493–498 (2019).\n\n\n11. A. S. Morcos, C. D. Harvey, History-dependent variability in population dynamics during evidence accumulation in cortex. Nature Neuroscience. 19, 1672–1681 (2016).\n\n\n12. T. D. Kim, T. Z. Luo, J. W. Pillow, C. D. Brody, \"Inferring Latent Dynamics Underlying Neural Population Activity via Neural Differential Equations\" in Proceedings of the 38th International Conference on Machine Learning (PMLR, 2021), pp. 5551–5561.\n\n\n13. D. Sussillo, O. Barak, Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks. Neural Computation. 25, 626–649 (2013).\n\n\n14. D. Sussillo, Neural circuits as computational dynamical systems. Current Opinion in Neurobiology. 25, 156–163 (2014).\n\n\n15. J. T. H. Smith, S. W. Linderman, D. Sussillo, \"Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems\" in Advances in Neural Information Processing Systems (2021; https://arxiv.org/abs/2111.01256), vol. 20, pp. 16700–16713.\n\n\n16. S. Vyas, M. D. Golub, D. Sussillo, K. V. Shenoy, Computation through Neural Population Dynamics. Annual Review of Neuroscience. 43, 249–275 (2020).\n\n\n17. J. Pearl, Causality (Cambridge University Press, New York, 2009).\n\n\n18. S. Dufour, Y. D. Koninck, Optrodes for combined optogenetics and electrophysiology in live animals. Neurophotonics. 2, 031205 (2015).\n\n\n19. K. Y. Kwon, H.-M. Lee, M. Ghovanloo, A. Weber, W. Li, Design, fabrication, and packaging of an integrated, wirelessly-powered optrode array for optogenetics application. Frontiers in Systems Neuroscience. 9 (2015).\n\n\n20. M. Welkenhuysen, L. Hoffman, Z. Luo, A. De Proft, C. Van den Haute, V. Baekelandt, Z. Debyser, G. Gielen, R. Puers, D. Braeken, An integrated multi-electrode-optrode array for in vitro optogenetics. Scientific Reports. 6, 20353 (2016).\n\n\n21. L. Wang, K. Huang, C. Zhong, L. Wang, Y. Lu, Fabrication and modification of implantable optrode arrays for in vivo optogenetic applications. Biophysics Reports. 4, 82–93 (2018).\n\n\n22. N. McAlinden, Y. Cheng, R. Scharf, E. Xie, E. Gu, C. F. Reiche, R. Sharma, P. Tathireddy, P. Tathireddy, L. Rieth, S. Blair, K. Mathieson, Multisite microLED optrode array for neural interfacing. Neurophotonics. 6, 035010 (2019).\n\n\n23. D. Mao, N. Li, Z. Xiong, Y. Sun, G. Xu, Single-Cell Optogenetic Control of Calcium Signaling with a High-Density Micro-LED Array. iScience. 21, 403–412 (2019).\n\n\n24. D. Mao, Z. Xiong, M. Donnelly, G. Xu, Brushing-Assisted Two-Color Quantum-Dot Micro-LED Array Towards Bi-Directional Optogenetics. IEEE Electron Device Letters. 42, 1504–1507 (2021).\n\n\n25. Y. Ohta, M. C. Guinto, T. Tokuda, M. Kawahara, M. Haruta, H. Takehara, H. Tashiro, K. Sasagawa, H. Onoe, R. Yamaguchi, Y. Koshimizu, K. Isa, T. Isa, K. Kobayashi, Y. M. Akay, M. Akay, J. Ohta, Micro-LED Array-Based Photo-Stimulation Devices for Optogenetics in Rat and Macaque Monkey Brains. IEEE Access. 9, 127937–127949 (2021).\n\n\n26. J. Antolik, Q. Sabatier, C. Galle, Y. Frégnac, R. Benosman, Assessment of optogenetically-driven strategies for prosthetic restoration of cortical vision in large-scale neural simulation of V1. Scientific Reports. 11, 1–18 (2021).\n\n\n27. S. Jeon, Y. Lee, D. Ryu, Y. K. Cho, Y. Lee, S. B. Jun, C.-H. Ji, Implantable Optrode Array for Optogenetic Modulation and Electrical Neural Recording. Micromachines. 12, 725 (2021).\n\n\n28. C. Kathe, F. Michoud, P. Schönle, A. Rowald, N. Brun, J. Ravier, I. Furfaro, V. Paggi, K. Kim, S. Soloukey, L. Asboth, T. H. Hutson, I. Jelescu, A. Philippides, N. Alwahab, J. Gandar, D. Huber, C. I. De Zeeuw, Q. Barraud, Q. Huang, S. P. Lacour, G. Courtine, Wireless closed-loop optogenetics across the entire dorsoventral spinal cord in mice. Nature Biotechnology. 40, 198–208 (2022).\n\n\n29. D. Eriksson, A. Schneider, A. Thirumalai, M. Alyahyay, B. de la Crompe, K. Sharma, P. Ruther, I. Diester, Multichannel optogenetics combined with laminar recordings for ultra-controlled neuronal interrogation. Nature Communications. 13, 985 (2022).\n\n\n30. A. M. Packer, L. E. Russell, H. W. P. Dalgleish, M. Häusser, Simultaneous all-optical manipulation and recording of neural circuit activity with cellular resolution in vivo. Nature Methods. 12, 140–146 (2015).\n\n\n31. E. Ronzitti, R. Conti, V. Zampini, D. Tanese, A. J. Foust, N. Klapoetke, E. S. Boyden, E. Papagiakoumou, V. Emiliani, Submillisecond Optogenetic Control of Neuronal Firing with Two-Photon Holographic Photoactivation of Chronos. Journal of Neuroscience. 37, 10679–10689 (2017).\n\n\n32. I.-W. Chen, E. Papagiakoumou, V. Emiliani, Towards circuit optogenetics. Current Opinion in Neurobiology. 50, 179–189 (2018).\n\n\n33. Z. Zhang, L. E. Russell, A. M. Packer, O. M. Gauld, M. Häusser, Closed-loop all-optical interrogation of neural circuits in vivo. Nature Methods. 15, 1037–1040 (2018).\n\n\n34. S. Sridharan, M. A. Gajowa, M. B. Ogando, U. K. Jagadisan, L. Abdeladim, M. Sadahiro, H. A. Bounds, W. D. Hendricks, T. S. Turney, I. Tayler, K. Gopakumar, I. A. Oldenburg, S. G. Brohawn, H. Adesnik, High-performance microbial opsins for spatially and temporally precise perturbations of large neuronal networks. Neuron. 110, 1139–1155.e6 (2022).\n\n\n35. M. M. Churchland, J. P. Cunningham, M. T. Kaufman, S. I. Ryu, K. V. Shenoy, Cortical Preparatory Activity: Representation of Movement or First Cog in a Dynamical Machine? Neuron. 68, 387–400 (2010).\n\n\n36. J. A. Gallego, M. G. Perich, L. E. Miller, S. A. Solla, Neural Manifolds for the Control of Movement. Neuron. 94, 978–984 (2017).\n\n\n37. G. Santhanam, B. M. Yu, V. Gilja, S. I. Ryu, A. Afshar, M. Sahani, K. V. Shenoy, Factor-Analysis Methods for Higher-Performance Neural Prostheses. Journal of Neurophysiology. 102, 1315–1330 (2009).\n\n\n38. G. Bellec, F. Scherr, A. Subramoney, E. Hajek, D. Salaj, R. Legenstein, W. Maass, A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications. 11 (2020), doi:10.1101/738385.\n\n\n39. R. Rajalingham, A. Piccato, M. Jazayeri, Recurrent neural networks with explicit representation of dynamic latent variables can mimic behavioral patterns in a physical inference task. Nature Communications. 13, 1–15 (2022).\n\n\n40. G. R. Yang, M. R. Joglekar, H. F. Song, W. T. Newsome, X.-J. Wang, Task representations in neural networks trained to perform many cognitive tasks. Nature Neuroscience. 22, 297–306 (2019).\n\n\n41. L. Driscoll, K. Shenoy, D. Sussillo, Flexible multitask computation in recurrent networks utilizes shared dynamical motifs (2022), p. 2022.08.15.503870.\n\n\n42. K. V. Shenoy, J. C. Kao, Measurement, manipulation and modeling of brain-wide neural population dynamics. Nature Communications. 12, 1–5 (2021).\n\n\n43. L. Duncker, M. Sahani, Dynamics on the manifold: Identifying computational dynamical activity from neural population recordings. Current Opinion in Neurobiology. 70, 163–170 (2021).\n\n\n44. C. Rackauckas, Q. Nie, DifferentialEquations.jl A Performant and Feature-Rich Ecosystem for Solving Differential Equations in Julia. Journal of Open Research Software. 5, 15 (2017).\n\n\n45. F. Zenke, T. P. Vogels, The Remarkable Robustness of Surrogate Gradient Learning for Instilling Complex Function in Spiking Neural Networks. Neural Computation. 33, 899–925 (2021)."
  },
  {
    "objectID": "src/timeline.html",
    "href": "src/timeline.html",
    "title": "7  Proposed timeline",
    "section": "",
    "text": "%%{ init: {'fontFamily': 'Source Sans Pro, Open Sans, trebuchet ms, Arial, sans-serif'}}%%\ngantt\n    dateFormat  YYYY-MM-DD\n    axisFormat  %b '%y\n    %% title       Proposed timeline\n    excludes    weekends\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n\n    section Aim 1\n    edit paper            :active, a1edit, 2023-01-01, 2w\n    submission to eLife :milestone, a1submit, after a1edit, 3d\n    wait for review  :done, a1wait, after a1submit, 8w\n    paper revisions :a1revise, after a1wait, 6w\n\n    section Aim 2\n    %% refine control code :a2code, after f31, 4w\n    %% add multi-input opto to Cleo :a2multi, after a2code, 2w\n    design and run simulations :a2run, after f31, 10w\n    analyze results :a2analyze, after a2run a1revise intern, 4w\n    write paper :a2paper, after a2analyze, 8w\n    submission to JNE :milestone, a2submit, after a2paper, 3d\n    wait for review  :done, a2wait, after a2submit, 8w\n    paper revisions :a2revise, after a3train a2wait, 4w\n\n    section Aim 3\n    %% set up project infrastructure :a3infra, after a2submit, 4w\n    configure and train RSNN models : a3train, after a2submit, 12w\n    %% configure system ID and control methods :a3sysctrl, after a3train a2revise, 6w\n    perform system ID and control experiments :a3expt, after a3train a2revise, 9w\n    analyze data :a3analyze, after a3expt, 3w\n    write paper :a3paper, after a3analyze, 8w\n    submission to Nat. Neuro :milestone, a3submit, after a3paper, 3d\n\n\n    section Other\n    F31 submission :f31, after a1submit, 4w\n    possible internship :intern, 2023-05-01, 12w\n    prepare dissertation :prep, after a3submit, 8w\n    defend :milestone, after prep, 2d"
  },
  {
    "objectID": "src/references.html",
    "href": "src/references.html",
    "title": "References",
    "section": "",
    "text": "1. H.\nAdesnik, L. Abdeladim, Probing neural codes\nwith two-photon holographic optogenetics. Nature\nNeuroscience. 24, 1356–1366 (2021).\n\n\n2. T.\nAkam, D. M. Kullmann, Oscillatory multiplexing of\npopulation codes for selective communication in the mammalian brain.\nNature Reviews Neuroscience. 15, 111–122\n(2014).\n\n\n3. A.\nAlegria, A. Joshi, J. O’Brien, S. B. Kodandaramaiah, Single neuron recording:\nProgress towards high-throughput analysis. Bioelectronics in\nMedicine. 3, 33–36 (2020).\n\n\n4. J.\nAntolik, Q. Sabatier, C. Galle, Y. Frégnac, R. Benosman, Assessment of\noptogenetically-driven strategies for prosthetic restoration of cortical\nvision in large-scale neural simulation of V1.\nScientific Reports. 11, 1–18 (2021).\n\n\n5. G.\nK. Anumanchipalli, J. Chartier, E. F. Chang, Speech synthesis from\nneural decoding of spoken sentences. Nature.\n568, 493–498 (2019).\n\n\n6. J.\nAru, J. Aru, V. Priesemann, M. Wibral, L. Lana, G. Pipa, W. Singer, R.\nVicente, Untangling\ncross-frequency coupling in neuroscience. Current Opinion in\nNeurobiology. 31, 51–61 (2015).\n\n\n7. A.\nAussel, L. Buhry, L. Tyvaert, R. Ranta, A detailed anatomical\nand mathematical model of the hippocampal formation for the generation\nof sharp-wave ripples and theta-nested gamma oscillations.\nJournal of Computational Neuroscience. 45,\n207–221 (2018).\n\n\n8. A.\nAussel, R. Ranta, O. Aron, S. Colnat-Coulbois, L. Maillard, L. Buhry,\nCell to network computational model of the epileptic human hippocampus\nsuggests specific roles of network and channel dysfunctions in the ictal\nand interictal oscillations. Journal of Computational\nNeuroscience (2022), doi:10.1007/s10827-022-00829-5.\n\n\n9. L.\nAvitan, C. Stringer, Not so spontaneous: Multi-dimensional representations of behaviors and\ncontext in sensory areas. Neuron (2022), doi:10.1016/j.neuron.2022.06.019.\n\n\n10. D.\nL. Barack, J. W. Krakauer, Two views on the\ncognitive brain. Nature Reviews Neuroscience.\n22, 359–371 (2021).\n\n\n11. G.\nBellec, F. Scherr, A. Subramoney, E. Hajek, D. Salaj, R. Legenstein, W.\nMaass, A solution to the learning dilemma for recurrent networks of\nspiking neurons. Nature Communications. 11\n(2020), doi:10.1101/738385.\n\n\n12. A.\nBemporad, M. Morari, V. Dua, E. N. Pistikopoulos, The explicit linear\nquadratic regulator for constrained systems. Automatica.\n38, 3–20 (2002).\n\n\n13. G.\nJ. Berman, D. M. Choi, W. Bialek, J. W. Shaevitz, Mapping the stereotyped\nbehaviour of freely moving fruit flies. Journal of The Royal\nSociety Interface. 11, 20140672 (2014).\n\n\n14. A.\nBogdanchikov, M. Zhaparov, R. Suliyev, \"Python to learn\nprogramming\" in Journal of Physics:\nConference Series (IOP Publishing, 2013),\nvol. 423, p. 012027.\n\n\n15. M.\nF. Bolus, A. A. Willats, C. J. Whitmire, C. J. Rozell, G. B. Stanley, Design strategies for\ndynamic closed-loop optogenetic neurocontrol in vivo. Journal of\nNeural Engineering. 15, 026011 (2018).\n\n\n16. M.\nF. Bolus, A. A. Willats, C. J. Rozell, G. B. Stanley, State-space optimal\nfeedback control of optogenetically driven neural activity.\nJournal of neural engineering. 18, 036006\n(2021).\n\n\n17. N.\nBrunel, X.-J. Wang, What determines the frequency of fast network\noscillations with irregular neural discharges? I.\nSynaptic dynamics and excitation-inhibition balance.\nJournal of neurophysiology. 90, 415–430\n(2003).\n\n\n18. E.\nA. Buffalo, P. Fries, R. Landman, T. J. Buschman, R. Desimone, Laminar differences in\ngamma and alpha coherence in the ventral stream. Proceedings of\nthe National Academy of Sciences of the United States of America.\n108, 11262–11267 (2011).\n\n\n19. T.\nJ. Buschman, E. L. Denovellis, C. Diogo, D. Bullock, E. K. Miller, Synchronous\nOscillatory Neural Ensembles for Rules in the\nPrefrontal Cortex. Neuron.\n76, 838–846 (2012).\n\n\n20. G.\nBuzsáki, A. Draguhn, Neuronal oscillations in\ncortical networks. Science. 304, 1926–1929\n(2004).\n\n\n21. G.\nBuzsáki, C. A. Anastassiou, C. Koch, The origin of extracellular\nfields and currents EEG, ECoG,\nLFP and spikes. Nature Reviews Neuroscience 2012\n13:6. 13, 407–420 (2012).\n\n\n22. G.\nBuzsáki, X.-J. Wang, Mechanisms of\nGamma Oscillations. Annual Review of\nNeuroscience. 35, 203–225 (2012).\n\n\n23. G.\nBuzsáki, Hippocampal sharp\nwave-ripple: A cognitive biomarker for episodic memory and\nplanning. Hippocampus. 25, 1073–1188\n(2015).\n\n\n24. J.\nA. Cardin, M. Carlén, K. Meletis, U. Knoblich, F. Zhang, K. Deisseroth,\nL. H. Tsai, C. I. Moore, Targeted optogenetic\nstimulation and recording of neurons in vivo using cell-type-specific\nexpression of Channelrhodopsin-2. Nature\nProtocols. 5, 247–254 (2010).\n\n\n25. S.\nChen, A. Z. Weitemier, X. Zeng, L. He, X. Wang, Y. Tao, A. J. Y. Huang,\nY. Hashimotodani, M. Kano, H. Iwasaki, L. K. Parajuli, S. Okabe, D. B.\nLoong Teh, A. H. All, I. Tsutsui-Kimura, K. F. Tanaka, X. Liu, T. J.\nMcHugh, Near-infrared\ndeep brain stimulation via upconversion nanoparticlemediated\noptogenetics. Science. 359, 679–684\n(2018).\n\n\n26. I.-W. Chen, E. Papagiakoumou, V. Emiliani, Towards circuit\noptogenetics. Current Opinion in Neurobiology.\n50, 179–189 (2018).\n\n\n27. M.\nM. Churchland, J. P. Cunningham, M. T. Kaufman, S. I. Ryu, K. V. Shenoy,\nCortical\nPreparatory Activity: Representation\nof Movement or First Cog in a Dynamical\nMachine? Neuron. 68, 387–400\n(2010).\n\n\n28. M.\nM. Churchland, J. P. Cunningham, M. T. Kaufman, J. D. Foster, P.\nNuyujukian, S. I. Ryu, K. V. Shenoy, K. V. Shenoy, Neural population dynamics\nduring reaching. Nature. 487, 51–56\n(2012).\n\n\n29. S.\nR. Cole, B. Voytek, Brain\nOscillations and the Importance of\nWaveform Shape. Trends in Cognitive Sciences.\n21, 137–149 (2017).\n\n\n30. S.\nCole, B. Voytek, Cycle-by-cycle analysis of\nneural oscillations. Journal of Neurophysiology.\n122, 849–861 (2019).\n\n\n31. B.\nR. Cowley, A. C. Snyder, K. Acar, R. C. Williamson, B. M. Yu, M. A.\nSmith, Slow\nDrift of Neural Activity as a\nSignature of Impulsivity in Macaque\nVisual and Prefrontal Cortex. Neuron.\n108, 551–567.e8 (2020).\n\n\n32. J.\nP. Cunningham, B. M. Yu, Dimensionality reduction for\nlarge-scale neural recordings. Nature neuroscience.\n17, 1500–9 (2014).\n\n\n33. J.\nT. Davie, M. H. P. Kole, J. J. Letzkus, E. A. Rancz, N. Spruston, G. J.\nStuart, M. Häusser, Dendritic patch-clamp\nrecording. Nature Protocols. 1, 1235–1247\n(2006).\n\n\n34. Z.\nW. Davis, L. Muller, J. Martinez-Trujillo, T. Sejnowski, J. H. Reynolds,\nSpontaneous\ntravelling cortical waves gate perception in behaving primates.\nNature. 587, 432–436 (2020).\n\n\n35. A.\nP. Davison, M. L. Hines, E. Muller, Trends in programming\nlanguages for neuroscience simulations. Frontiers in\nNeuroscience. 3, 374–380 (2009).\n\n\n36. L.\nDriscoll, K. Shenoy, D. Sussillo, Flexible multitask\ncomputation in recurrent networks utilizes shared dynamical motifs\n(2022), p. 2022.08.15.503870.\n\n\n37. S.\nDufour, Y. D. Koninck, Optrodes for combined\noptogenetics and electrophysiology in live animals.\nNeurophotonics. 2, 031205 (2015).\n\n\n38. L.\nDuncker, M. Sahani, Dynamics on the\nmanifold: Identifying computational dynamical activity from\nneural population recordings. Current Opinion in\nNeurobiology. 70, 163–170 (2021).\n\n\n39. S.\nDutta, E. Ackermann, C. Kemere, Analysis of an open\nsource, closed-loop, realtime system for hippocampal sharp-wave ripple\ndisruption. Journal of Neural Engineering.\n16, 016009 (2019).\n\n\n40. V.\nEmiliani, A. E. Cohen, K. Deisseroth, M. Häusser, All-optical\ninterrogation of neural circuits. Journal of Neuroscience.\n35, 13917–13926 (2015).\n\n\n41. D.\nEngel, Subcellular patch-clamp\nrecordings from the somatodendritic domain of nigral dopamine\nneurons. Journal of Visualized Experiments.\n2016, e54601 (2016).\n\n\n42. D.\nEriksson, A. Schneider, A. Thirumalai, M. Alyahyay, B. de la Crompe, K.\nSharma, P. Ruther, I. Diester, Multichannel\noptogenetics combined with laminar recordings for ultra-controlled\nneuronal interrogation. Nature Communications.\n13, 985 (2022).\n\n\n43. B.\nD. Evans, S. Jarvis, S. R. Schultz, K. Nikolic, PyRhO:\nA Multiscale Optogenetics Simulation Platform.\nFrontiers in Neuroinformatics. 10, 8\n(2016).\n\n\n44. M.\nS. Fabus, A. J. Quinn, C. E. Warnaby, M. W. Woolrich, Automatic decomposition of\nelectrophysiological data into distinct nonsinusoidal oscillatory\nmodes. Journal of Neurophysiology. 126,\n1670–1684 (2021).\n\n\n45. G.\nFaini, C. Molinier, C. Telliez, C. Tourain, B. C. Forget, E. Ronzitti,\nV. Emiliani, Ultrafast Light\nTargeting for High-Throughput Precise Control of\nNeuronal Networks. bioRxiv, 2021.06.14.448315\n(2021).\n\n\n46. L.\nFenno, O. Yizhar, K. Deisseroth, The\ndevelopment and application of optogenetics. Annual Review of\nNeuroscience. 34, 389–412 (2011).\n\n\n47. L.\nM. J. Fernandez, A. Lüthi, Sleep\nSpindles: Mechanisms and\nFunctions. Physiological Reviews.\n100, 805–868 (2020).\n\n\n48. N.\nC. Flytzanis, C. N. Bedbrook, H. Chiu, M. K. M. Engqvist, C. Xiao, K. Y.\nChan, P. W. Sternberg, F. H. Arnold, V. Gradinaru, Archaerhodopsin variants with\nenhanced voltage-sensitive fluorescence in mammalian and\nCaenorhabditis elegans neurons. Nature\nCommunications. 5, 4894 (2014).\n\n\n49. T.\nJ. Foutz, R. L. Arlow, C. C. Mcintyre, Theoretical\nprinciples underlying optical stimulation of a channelrhodopsin-2\npositive pyramidal neuron. J Neurophysiol.\n107, 3235–3245 (2012).\n\n\n50. J.\nA. Gallego, M. G. Perich, L. E. Miller, S. A. Solla, Neural\nManifolds for the Control of\nMovement. Neuron. 94, 978–984\n(2017).\n\n\n51. W.\nGerstner, W. M. Kistler, R. Naud, L. Paninski, Neuronal dynamics:\nFrom single neurons to networks and models of\ncognition (Cambridge University Press,\n2014).\n\n\n52. W.\nGöbel, F. Helmchen, In Vivo Calcium\nImaging of Neural Network Function.\nPhysiology. 22, 358–365 (2007).\n\n\n53. L.\nGrosenick, J. H. Marshel, K. Deisseroth, Review\nClosed-Loop and Activity-Guided Optogenetic\nControl. Neuron. 86, 106–139\n(2015).\n\n\n54. P.\nGutruf, J. A. Rogers, Implantable, wireless\ndevice platforms for neuroscience research. Current Opinion in\nNeurobiology. 50, 42–49 (2018).\n\n\n55. E.\nHagen, S. Næss, T. V. Ness, G. T. Einevoll, Multimodal modeling of\nneural network activity: Computing LFP, ECoG,\nEEG, and MEG signals with LFPy\n2.0. Frontiers in Neuroinformatics. 12, 92\n(2018).\n\n\n56. D.\nR. Hochbaum, Y. Zhao, S. L. Farhi, N. Klapoetke, C. A. Werley, V.\nKapoor, P. Zou, J. M. Kralj, D. MacLaurin, N. Smedemark-Margulies, J. L.\nSaulnier, G. L. Boulting, C. Straub, Y. K. Cho, M. Melkonian, G. K. S.\nWong, D. J. Harrison, V. N. Murthy, B. L. Sabatini, E. S. Boyden, R. E.\nCampbell, A. E. Cohen, All-optical electrophysiology\nin mammalian neurons using engineered microbial rhodopsins.\nNature Methods. 11, 825–833 (2014).\n\n\n57. A.\nL. Hodgkin, A. F. Huxley, B. Katz, Measurement\nof current-voltage relations in the membrane of the giant axon of\nLoligo. The Journal of Physiology.\n116, 424–448 (1952).\n\n\n58. G.\nR. Holt, C. Koch, Electrical interactions\nvia the extracellular potential near cell bodies. Journal of\nComputational Neuroscience. 6, 169–184\n(1999).\n\n\n59. C.\nHurwitz, A. Srivastava, K. Xu, J. Jude, M. G. Perich, L. E. Miller, M.\nH. Hennig, \"Targeted Neural Dynamical Modeling\" in\nAdvances in Neural Information Processing Systems\n(2021; https://arxiv.org/abs/2110.14853),\nvol. 35, pp. 29379–29392.\n\n\n60. M.\nInoue, Genetically encoded\ncalcium indicators to probe complex brain circuit dynamics in vivo.\nNeuroscience Research. 169, 2–8 (2021).\n\n\n61. M.\nJazayeri, S. Ostojic, Interpreting neural\ncomputations by examining intrinsic and embedding dimensionality of\nneural activity. Current Opinion in Neurobiology.\n70, 113–120 (2021).\n\n\n62. S.\nJeon, Y. Lee, D. Ryu, Y. K. Cho, Y. Lee, S. B. Jun, C.-H. Ji, Implantable Optrode\nArray for Optogenetic Modulation and\nElectrical Neural Recording. Micromachines.\n12, 725 (2021).\n\n\n63. A.\nJoglekar, A. Prjibelski, A. Mahfouz, P. Collier, S. Lin, A. K.\nSchlusche, J. Marrocco, S. R. Williams, B. Haase, A. Hayes, J. G. Chew,\nN. I. Weisenfeld, M. Y. Wong, A. N. Stein, S. A. Hardwick, T. Hunt, Q.\nWang, C. Dieterich, Z. Bent, O. Fedrigo, S. A. Sloan, D. Risso, E. D.\nJarvis, P. Flicek, W. Luo, G. S. Pitt, A. Frankish, A. B. Smit, M. E.\nRoss, H. U. Tilgner, A spatially resolved\nbrain region- and cell type-specific isoform atlas of the postnatal\nmouse brain. Nature Communications. 12,\n463 (2021).\n\n\n64. K.\nJohnsen, Kjohnsen/tklfp: V0.2.0 (2022), doi:10.5281/zenodo.6787979.\n\n\n65. A.\nL. Juavinett, G. Bekheet, A. K. Churchland, Chronically implanted\nNeuropixels probes enable high-yield recordings in freely\nmoving mice. eLife. 8, e47188\n(2019).\n\n\n66. J.\nF. Kalaska, R. Caminiti, A. P. Georgopoulos, Cortical mechanisms related to\nthe direction of two-dimensional arm movements: Relations in parietal\narea 5 and comparison with motor cortex. Experimental Brain\nResearch. 51, 247–260 (1983).\n\n\n67. R.\nE. Kalman, A new approach to\nlinear filtering and prediction problems. Journal of Fluids\nEngineering, Transactions of the ASME. 82, 35–45\n(1960).\n\n\n68. G.\nKarvat, A. Schneider, M. Alyahyay, F. Steenbergen, M. Tangermann, I.\nDiester, Real-time\ndetection of neural oscillation bursts allows behaviourally relevant\nneurofeedback. Communications Biology. 3,\n1–10 (2020).\n\n\n69. C.\nKathe, F. Michoud, P. Schönle, A. Rowald, N. Brun, J. Ravier, I.\nFurfaro, V. Paggi, K. Kim, S. Soloukey, L. Asboth, T. H. Hutson, I.\nJelescu, A. Philippides, N. Alwahab, J. Gandar, D. Huber, C. I. De\nZeeuw, Q. Barraud, Q. Huang, S. P. Lacour, G. Courtine, Wireless closed-loop\noptogenetics across the entire dorsoventral spinal cord in mice.\nNature Biotechnology. 40, 198–208\n(2022).\n\n\n70. M.\nT. Kaufman, M. M. Churchland, S. I. Ryu, K. V. Shenoy, Cortical activity in the null\nspace: Permitting preparation without movement.\nNature Neuroscience. 17, 440–448 (2014).\n\n\n71. A.\nKazemipour, O. Novak, D. Flickinger, J. S. Marvin, A. S. Abdelfattah, J.\nKing, P. M. Borden, J. J. Kim, S. H. Al-Abdullatif, P. E. Deal, E. W.\nMiller, E. R. Schreiter, S. Druckmann, K. Svoboda, L. L. Looger, K.\nPodgorski, Kilohertz\nframe-rate two-photon tomography. Nature Methods.\n16, 778–786 (2019).\n\n\n72. T.\nD. Kim, T. Z. Luo, J. W. Pillow, C. D. Brody, \"Inferring Latent\nDynamics Underlying Neural Population Activity via Neural\nDifferential Equations\" in Proceedings of the 38th\nInternational Conference on Machine\nLearning (PMLR, 2021), pp. 5551–5561.\n\n\n73. K.\nE. Kishi, Y. S. Kim, M. Fukuda, M. Inoue, T. Kusakizako, P. Y. Wang, C.\nRamakrishnan, E. F. X. Byrne, E. Thadhani, J. M. Paggi, T. E. Matsui, K.\nYamashita, T. Nagata, M. Konno, S. Quirin, M. Lo, T. Benster, T. Uemura,\nK. Liu, M. Shibata, N. Nomura, S. Iwata, O. Nureki, R. O. Dror, K.\nInoue, K. Deisseroth, H. E. Kato, Structural basis for\nchannel conduction in the pump-like channelrhodopsin\nChRmine. Cell. 185,\n672–689.e23 (2022).\n\n\n74. T.\nKnöpfel, C. Song, Optical voltage imaging\nin neurons: Moving from technology development to practical tool.\nNature Reviews Neuroscience. 20, 719–727\n(2019).\n\n\n75. M.\nKokaia, M. Andersson, M. Ledri, An optogenetic\napproach in epilepsy. Neuropharmacology.\n69, 89–95 (2013).\n\n\n76. E.\nKrook-Magnuson, C. Armstrong, M. Oijala, I. Soltesz, On-demand optogenetic control\nof spontaneous seizures in temporal lobe epilepsy. Nature\nCommunications. 4, 1–8 (2013).\n\n\n77. A.\nKumar, I. Vlachos, A. Aertsen, C. Boucsein, Challenges of\nunderstanding brain function by selective modulation of neuronal\nsubpopulations. Trends in Neurosciences.\n36, 579–586 (2013).\n\n\n78. K.\nY. Kwon, H.-M. Lee, M. Ghovanloo, A. Weber, W. Li, Design, fabrication,\nand packaging of an integrated, wirelessly-powered optrode array for\noptogenetics application. Frontiers in Systems Neuroscience.\n9 (2015).\n\n\n79. C.\nLee, A. Lavoie, J. Liu, S. X. Chen, B. Liu, Light Up the\nBrain: The Application of\nOptogenetics in Cell-Type Specific Dissection\nof Mouse Brain Circuits. Frontiers in Neural\nCircuits. 14 (2020).\n\n\n80. J.\nY. Lin, P. M. Knutsen, A. Muller, D. Kleinfeld, R. Y. Tsien, ReaChR:\nA red-shifted variant of channelrhodopsin enables deep\ntranscranial optogenetic excitation. Nature Neuroscience.\n16, 1499–1508 (2013).\n\n\n81. M.\nLundqvist, J. Rose, P. Herman, S. L. L. Brincat, T. J. J. Buschman, E.\nK. K. Miller, Gamma and Beta\nBursts Underlie Working Memory. Neuron.\n90, 152–164 (2016).\n\n\n82. M.\nLundqvist, J. Rose, S. L. Brincat, M. R. Warden, T. J. Buschman, P.\nHerman, E. K. Miller, Reduced variability of\nbursting activity during working memory. Scientific\nReports. 12, 15050 (2022).\n\n\n83. L.\nvan der Maaten, G. Hinton, Visualizing Data using\nt-SNE. Journal of Machine Learning Research.\n9, 2579–2605 (2008).\n\n\n84. J.\nH. Macke, L. Buesing, J. P. Cunningham, B. M. Yu, K. V. Shenoy, M.\nSahani, \"Empirical models of spiking in neural populations\" in\nAdvances in Neural Information Processing Systems\n(Curran Associates, Inc., 2011), vol. 24.\n\n\n85. D.\nMao, N. Li, Z. Xiong, Y. Sun, G. Xu, Single-Cell\nOptogenetic Control of Calcium Signaling with a\nHigh-Density Micro-LED Array. iScience.\n21, 403–412 (2019).\n\n\n86. D.\nMao, Z. Xiong, M. Donnelly, G. Xu, Brushing-Assisted\nTwo-Color Quantum-Dot Micro-LED Array Towards Bi-Directional\nOptogenetics. IEEE Electron Device Letters.\n42, 1504–1507 (2021).\n\n\n87. R.\nI. Martinez-Garcia, B. Voelcker, J. B. Zaltsman, S. L. Patrick, T. R.\nStevens, B. W. Connors, S. J. Cruikshank, Two dynamically\ndistinct circuits drive inhibition in the sensory thalamus.\nNature. 583, 813–818 (2020).\n\n\n88. A.\nMathis, P. Mamidanna, K. M. Cury, T. Abe, V. N. Murthy, M. W. Mathis, M.\nBethge, DeepLabCut:\nMarkerless pose estimation of user-defined body parts with deep\nlearning. Nature Neuroscience. 21,\n1281–1289 (2018).\n\n\n89. A.\nMazzoni, H. Lindén, H. Cuntz, A. Lansner, S. Panzeri, G. T. Einevoll, Computing the\nLocal Field Potential (LFP) from Integrate-and-Fire Network Models. PLOS\nComputational Biology. 11, e1004584 (2015).\n\n\n90. N.\nMcAlinden, Y. Cheng, R. Scharf, E. Xie, E. Gu, C. F. Reiche, R. Sharma,\nP. Tathireddy, P. Tathireddy, L. Rieth, S. Blair, K. Mathieson, Multisite microLED optrode array for neural interfacing.\nNeurophotonics. 6, 035010 (2019).\n\n\n91. S.\nMoldakarimov, M. Bazhenov, D. E. Feldman, T. J. Sejnowski, Structured networks\nsupport sparse traveling waves in rodent somatosensory cortex.\nProceedings of the National Academy of Sciences of the United States\nof America. 115, 5277–5282 (2018).\n\n\n92. A.\nS. Morcos, C. D. Harvey, History-dependent variability in\npopulation dynamics during evidence accumulation in cortex.\nNature Neuroscience. 19, 1672–1681\n(2016).\n\n\n93. E.\nA. Mukamel, J. Ngai, Perspectives on\ndefining cell types in the brain. Current Opinion in\nNeurobiology. 56, 61–68 (2019).\n\n\n94. E.\nMuller, J. A. Bednar, M. Diesmann, M. O. Gewaltig, M. Hines, A. P.\nDavison, Python in\nneuroscience. Frontiers in Neuroinformatics.\n9, 11 (2015).\n\n\n95. L.\nMuller, F. Chavane, J. Reynolds, T. J. Sejnowski, Cortical travelling waves:\nMechanisms and computational principles. Nature\nReviews Neuroscience. 19, 255–268 (2018).\n\n\n96. S.\nR. Nason, A. K. Vaskov, M. S. Willsey, E. J. Welle, H. An, P. P. Vu, A.\nJ. Bullard, C. S. Nu, J. C. Kao, K. V. Shenoy, T. Jang, H.-S. Kim, D.\nBlaauw, P. G. Patil, C. A. Chestek, A low-power band of\nneuronal spiking activity dominated by local single units improves the\nperformance of brainmachine interfaces. Nature Biomedical\nEngineering 2020, 1–11 (2020).\n\n\n97. P.\nNěmec, P. Osten, The evolution of brain\nstructure captured in stereotyped cell count and cell type\ndistributions. Current Opinion in Neurobiology.\n60, 176–183 (2020).\n\n\n98. J.\nNewman, R. Zeller-Townson, M. Fong, S. Arcot Desai, R. Gross, S. Potter,\nClosed-Loop, Multichannel Experimentation\nUsing the Open-Source NeuroRighter Electrophysiology\nPlatform. Frontiers in Neural Circuits.\n6 (2013).\n\n\n99. J.\nP. Newman, M. F. Fong, D. C. Millard, C. J. Whitmire, G. B. Stanley, S.\nM. Potter, Optogenetic feedback control of neural activity.\neLife (2015), doi:10.7554/eLife.07192.\n\n\n100. E.\nR. Oby, M. D. Golub, J. A. Hennig, A. D. Degenhart, E. C. Tyler-Kabara,\nB. M. Yu, S. M. Chase, A. P. Batista, New neural activity\npatterns emerge with long-term learning. Proceedings of the\nNational Academy of Sciences. 116, 15210–15215\n(2019).\n\n\n101. Y.\nOhta, M. C. Guinto, T. Tokuda, M. Kawahara, M. Haruta, H. Takehara, H.\nTashiro, K. Sasagawa, H. Onoe, R. Yamaguchi, Y. Koshimizu, K. Isa, T.\nIsa, K. Kobayashi, Y. M. Akay, M. Akay, J. Ohta, Micro-LED\nArray-Based Photo-Stimulation Devices for\nOptogenetics in Rat and Macaque Monkey\nBrains. IEEE Access. 9,\n127937–127949 (2021).\n\n\n102. A.\nM. Packer, B. Roska, M. Häuser, Targeting neurons and photons for\noptogenetics. Nature Neuroscience. 16,\n805–815 (2013).\n\n\n103. A.\nM. Packer, L. E. Russell, H. W. P. Dalgleish, M. Häusser, Simultaneous all-optical\nmanipulation and recording of neural circuit activity with cellular\nresolution in vivo. Nature Methods. 12,\n140–146 (2015).\n\n\n104. C.\nPandarinath, D. J. O’Shea, J. Collins, R. Jozefowicz, S. D. Stavisky, J.\nC. Kao, E. M. Trautmann, M. T. Kaufman, S. I. Ryu, L. R. Hochberg, J. M.\nHenderson, K. V. Shenoy, L. F. Abbott, D. Sussillo, Inferring single-trial\nneural population dynamics using sequential auto-encoders.\nNature Methods. 15, 805–815 (2018).\n\n\n105. H.\nParasuram, B. Nair, E. D’Angelo, M. Hines, G. Naldi, S. Diwakar, Computational modeling\nof single neuron extracellular electric potentials and network local\nfield potentials using LFPsim. Frontiers in\nComputational Neuroscience. 10, 65 (2016).\n\n\n106. J.\nPearl, Causality\n(Cambridge University Press, New York,\n2009).\n\n\n107. D.\nPeixoto, J. R. Verhein, R. Kiani, J. C. Kao, P. Nuyujukian, C.\nChandrasekaran, J. Brown, S. Fong, S. I. Ryu, K. V. Shenoy, W. T.\nNewsome, Decoding\nand perturbing decision states in real time. Nature.\n591, 604–609 (2021).\n\n\n108. Y.\nPeng, F. X. Mittermaier, H. Planert, U. C. Schneider, H. Alle, J. R. P.\nGeiger, High-throughput\nmicrocircuit analysis of individual human brains through next-generation\nmultineuron patch-clamp. eLife. 8, e48178\n(2019).\n\n\n109. K.\nH. Pettersen, H. Lindén, A. M. Dale, G. T. Einevoll, Extracellular\nspikes and CSD. Handbook of neural activity\nmeasurement. 1, 92–135 (2012).\n\n\n110. S.\nM. Potter, A. El Hady, E. E. Fetz, Closed-loop neuroscience\nand neuroengineering. Frontiers in Neural Circuits.\n0, 115 (2014).\n\n\n111. A.\nA. Prinz, L. F. Abbott, E. Marder, The dynamic clamp\ncomes of age. Trends in Neurosciences. 27,\n218–224 (2004).\n\n\n112. M.\nPrsa, G. L. Galiñanes, D. Huber, Rapid\nIntegration of Artificial Sensory Feedback\nduring Operant Conditioning of Motor Cortex\nNeurons. Neuron. 93, 929–939.e6\n(2017).\n\n\n113. C.\nRackauckas, Q. Nie, DifferentialEquations.jl\nA Performant and Feature-Rich Ecosystem for\nSolving Differential Equations in Julia.\nJournal of Open Research Software. 5, 15\n(2017).\n\n\n114. R.\nRajalingham, A. Piccato, M. Jazayeri, Recurrent neural\nnetworks with explicit representation of dynamic latent variables can\nmimic behavioral patterns in a physical inference task. Nature\nCommunications. 13, 1–15 (2022).\n\n\n115. E.\nRonzitti, R. Conti, V. Zampini, D. Tanese, A. J. Foust, N. Klapoetke, E.\nS. Boyden, E. Papagiakoumou, V. Emiliani, Submillisecond\nOptogenetic Control of Neuronal Firing with\nTwo-Photon Holographic Photoactivation of\nChronos. Journal of Neuroscience.\n37, 10679–10689 (2017).\n\n\n116. B.\nL. Roth, DREADDs\nfor Neuroscientists. Neuron.\n89, 683–694 (2016).\n\n\n117. M.\nE. Rule, C. Vargas-Irwin, J. P. Donoghue, W. Truccolo, Phase reorganization leads\nto transient β-LFP spatial\nwave patterns in motor cortex during steady-state movement\npreparation. Journal of Neurophysiology.\n119, 2212–2228 (2018).\n\n\n118. A.\nB. Saleem, A. D. Lien, M. Krumin, B. Haider, M. R. Rosón, A. Ayaz, K.\nReinhold, L. Busse, M. Carandini, K. D. Harris, M. Carandini, Subcortical\nSource and Modulation of the Narrowband\nGamma Oscillation in Mouse Visual Cortex.\nNeuron. 93, 315–322 (2017).\n\n\n119. O.\nG. Sani, Y. Yang, M. B. Lee, H. E. Dawes, E. F. Chang, M. M. Shanechi,\nMood variations decoded from\nmulti-site intracranial human brain activity. Nature\nBiotechnology. 36, 954–961 (2018).\n\n\n120. O.\nG. Sani, B. Pesaran, M. M. Shanechi, M. Hsieh, Where is all the\nnonlinearity: Flexible nonlinear modeling of behaviorally relevant\nneural dynamics using recurrent neural networks. bioRxiv,\n2021.09.03.458628 (2021).\n\n\n121. O.\nG. Sani, H. Abbaspourazad, Y. T. Wong, B. Pesaran, M. M. Shanechi, Modeling behaviorally\nrelevant neural dynamics enabled by preferential subspace\nidentification. Nature Neuroscience. 24,\n140–149 (2021).\n\n\n122. G.\nSanthanam, B. M. Yu, V. Gilja, S. I. Ryu, A. Afshar, M. Sahani, K. V.\nShenoy, Factor-Analysis\nMethods for Higher-Performance Neural\nProstheses. Journal of Neurophysiology.\n102, 1315–1330 (2009).\n\n\n123. T.\nK. Sato, I. Nauhaus, M. Carandini, Traveling\nWaves in Visual Cortex. Neuron.\n75, 218–229 (2012).\n\n\n124. L.\nK. Scheffer, C. S. Xu, M. Januszewski, Z. Lu, S. Takemura, K. J.\nHayworth, G. B. Huang, K. Shinomiya, J. Maitlin-Shepard, S. Berg, J.\nClements, P. M. Hubbard, W. T. Katz, L. Umayam, T. Zhao, D. Ackerman, T.\nBlakely, J. Bogovic, T. Dolafi, D. Kainmueller, T. Kawase, K. A. Khairy,\nL. Leavitt, P. H. Li, L. Lindsey, N. Neubarth, D. J. Olbris, H. Otsuna,\nE. T. Trautman, M. Ito, A. S. Bates, J. Goldammer, T. Wolff, R.\nSvirskas, P. Schlegel, E. Neace, C. J. Knecht, C. X. Alvarado, D. A.\nBailey, S. Ballinger, J. A. Borycz, B. S. Canino, N. Cheatham, M. Cook,\nM. Dreher, O. Duclos, B. Eubanks, K. Fairbanks, S. Finley, N. Forknall,\nA. Francis, G. P. Hopkins, E. M. Joyce, S. Kim, N. A. Kirk, J. Kovalyak,\nS. A. Lauchie, A. Lohff, C. Maldonado, E. A. Manley, S. McLin, C.\nMooney, M. Ndama, O. Ogundeyi, N. Okeoma, C. Ordish, N. Padilla, C. M.\nPatrick, T. Paterson, E. E. Phillips, E. M. Phillips, N. Rampally, C.\nRibeiro, M. K. Robertson, J. T. Rymer, S. M. Ryan, M. Sammons, A. K.\nScott, A. L. Scott, A. Shinomiya, C. Smith, K. Smith, N. L. Smith, M. A.\nSobeski, A. Suleiman, J. Swift, S. Takemura, I. Talebi, D. Tarnogorska,\nE. Tenshaw, T. Tokhi, J. J. Walsh, T. Yang, J. A. Horne, F. Li, R.\nParekh, P. K. Rivlin, V. Jayaraman, M. Costa, G. S. Jefferis, K. Ito, S.\nSaalfeld, R. George, I. A. Meinertzhagen, G. M. Rubin, H. F. Hess, V.\nJain, S. M. Plaza, A\nconnectome and analysis of the adult Drosophila central\nbrain. eLife. 9, e57443 (2020).\n\n\n125. S.\nSchneider, J. H. Lee, M. W. Mathis, Learnable latent embeddings for\njoint behavioral and neural analysis (2022), doi:10.48550/arXiv.2204.00673.\n\n\n126. A.\nA. Sharp, M. B. O’Neil, L. F. Abbott, E. Marder, The dynamic clamp:\nArtificial conductances in biological neurons. Trends in\nNeurosciences. 16, 389–394 (1993).\n\n\n127. K.\nV. Shenoy, M. Sahani, M. M. Churchland, Cortical\nControl of Arm Movements: A Dynamical\nSystems Perspective. Annual Review of Neuroscience.\n36, 337–359 (2013).\n\n\n128. K.\nV. Shenoy, J. C. Kao, Measurement,\nmanipulation and modeling of brain-wide neural population dynamics.\nNature Communications. 12, 1–5 (2021).\n\n\n129. J.\nH. Siegle, A. C. López, Y. A. Patel, K. Abramov, S. Ohayon, J. Voigts,\nOpen\nEphys: An open-source, plugin-based platform for\nmultichannel electrophysiology. Journal of Neural\nEngineering. 14, 045003 (2017).\n\n\n130. J.\nT. H. Smith, S. W. Linderman, D. Sussillo, \"Reverse engineering\nrecurrent neural networks with Jacobian switching linear\ndynamical systems\" in Advances in Neural Information\nProcessing Systems (2021; https://arxiv.org/abs/2111.01256),\nvol. 20, pp. 16700–16713.\n\n\n131. O.\nSporns, Graph\ntheory methods: Applications in brain networks. Dialogues in\nClinical Neuroscience. 20, 111–121 (2018).\n\n\n132. S.\nSridharan, M. A. Gajowa, M. B. Ogando, U. K. Jagadisan, L. Abdeladim, M.\nSadahiro, H. A. Bounds, W. D. Hendricks, T. S. Turney, I. Tayler, K.\nGopakumar, I. A. Oldenburg, S. G. Brohawn, H. Adesnik, High-performance\nmicrobial opsins for spatially and temporally precise perturbations of\nlarge neuronal networks. Neuron. 110,\n1139–1155.e6 (2022).\n\n\n133. S.\nS. Srinivasan, B. E. Maimon, M. Diaz, H. Song, H. M. Herr, Closed-loop functional\noptogenetic stimulation. Nature Communications.\n9, 1–10 (2018).\n\n\n134. N.\nA. Steinmetz, C. Aydin, A. Lebedeva, M. Okun, M. Pachitariu, M. Bauza,\nM. Beau, J. Bhagat, C. Böhm, M. Broux, S. Chen, J. Colonell, R. J.\nGardner, B. Karsh, F. Kloosterman, D. Kostadinov, C. Mora-Lopez, J.\nO’Callaghan, J. Park, J. Putzeys, B. Sauerbrei, R. J. J. van Daal, A. Z.\nVollan, S. Wang, M. Welkenhuysen, Z. Ye, J. T. Dudman, B. Dutta, A. W.\nHantman, K. D. Harris, A. K. Lee, E. I. Moser, J. O’Keefe, A. Renart, K.\nSvoboda, M. Häusser, S. Haesler, M. Carandini, T. D. Harris, Neuropixels\n2.0: A miniaturized high-density probe for stable,\nlong-term brain recordings. Science. 372\n(2021), doi:10.1126/science.abf4588.\n\n\n135. M.\nStimberg, R. Brette, D. F. M. Goodman, Brian 2, an intuitive and\nefficient neural simulator. eLife. 8 (2019),\ndoi:10.7554/eLife.47314.\n\n\n136. J.\nM. Stujenske, T. Spellman, J. A. Gordon, Modeling the\nSpatiotemporal Dynamics of Light and\nHeat Propagation for InVivo Optogenetics.\nCell Reports. 12, 525–534 (2015).\n\n\n137. D.\nSussillo, O. Barak, Opening the Black\nBox: Low-Dimensional Dynamics in\nHigh-Dimensional Recurrent Neural Networks. Neural\nComputation. 25, 626–649 (2013).\n\n\n138. D.\nSussillo, Neural\ncircuits as computational dynamical systems. Current Opinion in\nNeurobiology. 25, 156–163 (2014).\n\n\n139. K.\nSvoboda, R. Yasuda, Principles of\nTwo-Photon Excitation Microscopy and Its\nApplications to Neuroscience. Neuron.\n50, 823–839 (2006).\n\n\n140. S.\nTafazoli, C. J. MacDowell, Z. Che, K. C. Letai, C. R. Steinhardt, T. J.\nBuschman, Learning to\ncontrol the brain through adaptive closed-loop patterned\nstimulation. Journal of Neural Engineering.\n17, 056007 (2020).\n\n\n141. I.\nTal, S. Neymotin, S. Bickel, P. Lakatos, C. E. Schroeder, Oscillatory\nBursting as a Mechanism for Temporal\nCoupling and Information Coding. Frontiers in\nComputational Neuroscience. 14 (2020).\n\n\n142. B.\nTelenczuk, M. Telenczuk, A. Destexhe, A kernel-based\nmethod to calculate local field potentials from networks of spiking\nneurons. Journal of Neuroscience Methods.\n344, 108871 (2020).\n\n\n143. C.\nThornton, F. Hutchings, M. Kaiser, The virtual electrode recording tool\nfor extracellular potentials (VERTEX) Version\n2.0: Modelling in vitro electrical stimulation of brain\ntissue. Wellcome Open Research. 4 (2019),\ndoi:10.12688/wellcomeopenres.15058.1.\n\n\n144. R.\nJ. Tomsett, M. Ainsworth, A. Thiele, M. Sanayei, X. Chen, M. A.\nGieselmann, M. A. Whittington, M. O. Cunningham, M. Kaiser, Virtual Electrode\nRecording Tool for EXtracellular potentials\n(VERTEX): Comparing multi-electrode recordings from\nsimulated and biological mammalian cortical tissue. Brain\nStructure and Function. 220, 2333–2353\n(2015).\n\n\n145. A.\nR. Vaidya, M. S. Pujara, M. Petrides, E. A. Murray, L. K. Fellows, Lesion\nStudies in Contemporary Neuroscience.\nTrends in Cognitive Sciences. 23, 653–671\n(2019).\n\n\n146. J.\nVierock, S. Rodriguez-Rozada, A. Dieter, F. Pieper, R. Sims, F.\nTenedini, A. C. F. Bergs, I. Bendifallah, F. Zhou, N. Zeitzschel, J.\nAhlbeck, S. Augustin, K. Sauter, E. Papagiakoumou, A. Gottschalk, P.\nSoba, V. Emiliani, A. K. Engel, P. Hegemann, J. S. Wiegert, BiPOLES\nis an optogenetic tool developed for bidirectional dual-color control of\nneurons. Nature Communications. 12, 1–20\n(2021).\n\n\n147. T.\nVo-Dinh, Biomedical\nPhotonics: Handbook (CRC\nPress, 2003).\n\n\n148. S.\nVyas, M. D. Golub, D. Sussillo, K. V. Shenoy, Computation\nthrough Neural Population Dynamics. Annual Review\nof Neuroscience. 43, 249–275 (2020).\n\n\n149. D.\nA. Wagenaar, R. Madhavan, J. Pine, S. M. Potter, Controlling\nbursting in cortical cultures with closed-loop multi-electrode\nstimulation. Journal of Neuroscience. 25,\n680–688 (2005).\n\n\n150. Y.\nWang, S. Boyd, Fast\nModel Predictive Control Using Online Optimization.\nIEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY.\n18, 267 (2010).\n\n\n151. G.\nWang, D. R. Wyskiel, W. Yang, Y. Wang, L. C. Milbern, T. Lalanne, X.\nJiang, Y. Shen, Q. Q. Sun, J. J. Zhu, An optogenetics- and\nimaging-assisted simultaneous multiple patch-clamp recording system for\ndecoding complex neural circuits. Nature Protocols.\n10, 397–412 (2015).\n\n\n152. L.\nWang, K. Huang, C. Zhong, L. Wang, Y. Lu, Fabrication and\nmodification of implantable optrode arrays for in vivo optogenetic\napplications. Biophysics Reports. 4, 82–93\n(2018).\n\n\n153. X.-J. Wang, G. Buzsáki, Gamma oscillation by\nsynaptic inhibition in a hippocampal interneuronal network model.\nJournal of neuroscience. 16, 6402–6413\n(1996).\n\n\n154. M.\nWelkenhuysen, L. Hoffman, Z. Luo, A. De Proft, C. Van den Haute, V.\nBaekelandt, Z. Debyser, G. Gielen, R. Puers, D. Braeken, An integrated\nmulti-electrode-optrode array for in vitro optogenetics.\nScientific Reports. 6, 20353 (2016).\n\n\n155. J.\nS. Wiegert, M. Mahn, M. Prigge, Y. Printz, O. Yizhar, Silencing\nNeurons: Tools, Applications, and\nExperimental Constraints. Neuron.\n95, 504–529 (2017).\n\n\n156. F.\nR. Willett, D. T. Avansino, L. R. Hochberg, J. M. Henderson, K. V.\nShenoy, High-performance\nbrain-to-text communication via handwriting. Nature.\n593, 249–254 (2021).\n\n\n157. K.\nA. Wilmes, C. Clopath, Inhibitory\nmicrocircuits for top-down plasticity of sensory representations.\nNature Communications. 10, 5055 (2019).\n\n\n158. A.\nWitt, A. Palmigiano, A. Neef, A. El Hady, F. Wolf, D. Battaglia, Controlling the\noscillation phase through precisely timed closed-loop optogenetic\nstimulation: A computational study. Frontiers in Neural\nCircuits. 7, 1–17 (2013).\n\n\n159. J.\nWu, Y. Liang, S. Chen, C. L. Hsu, M. Chavarha, S. W. Evans, D. Shi, M.\nZ. Lin, K. K. Tsia, N. Ji, Kilohertz two-photon\nfluorescence microscopy imaging of neural activity in vivo.\nNature Methods. 17, 287–290 (2020).\n\n\n160. G.\nR. Yang, M. R. Joglekar, H. F. Song, W. T. Newsome, X.-J. Wang, Task representations in\nneural networks trained to perform many cognitive tasks. Nature\nNeuroscience. 22, 297–306 (2019).\n\n\n161. Y.\nYang, S. Qiao, O. G. Sani, J. I. Sedillo, B. Ferrentino, B. Pesaran, M.\nM. Shanechi, Modelling and\nprediction of the dynamic responses of large-scale brain networks during\ndirect electrical stimulation. Nature Biomedical\nEngineering. 5, 324–345 (2021).\n\n\n162. O.\nYizhar, L. E. Fenno, T. J. Davidson, M. Mogri, K. Deisseroth, Optogenetics in\nNeural Systems. Neuron. 71,\n9–34 (2011).\n\n\n163. H.\nZeng, What is a\ncell type and how to define it? Cell. 185,\n2739–2755 (2022).\n\n\n164. F.\nZenke, T. P. Vogels, The\nRemarkable Robustness of Surrogate Gradient\nLearning for Instilling Complex Function in\nSpiking Neural Networks. Neural Computation.\n33, 899–925 (2021).\n\n\n165. Z.\nZhang, L. E. Russell, A. M. Packer, O. M. Gauld, M. Häusser, Closed-loop all-optical\ninterrogation of neural circuits in vivo. Nature Methods.\n15, 1037–1040 (2018).\n\n\n166. L.\nZhang, J. Lee, C. Rozell, A. C. Singer, Sub-second dynamics of\ntheta-gamma coupling in hippocampal CA1. eLife.\n8 (2019), doi:10.7554/eLife.44320."
  }
]