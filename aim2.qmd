---
execute: 
  echo: false
---
# Aim 2: Multi-input CLOC {#sec-aim2}

<!-- math macros -->
\newcommand\RR{\mathbb{R}}
\newcommand\NN{\mathbb{N}}
\newcommand\mini{\text{minimize}}

## Rationale
### Advantages of multi-input control
The power of closed-loop optogenetic control (CLOC, henceforth referring specifically to feedback control; see @sec-cl-types) is limited by the degrees of freedom provided by the optogenetic actuation scheme.
One important distinction in possible actuation schemes is between unidirectional and bidirectional actuation, referring to whether a single opsin type or both excitatory and inhibitory opsins are used simultaneously.
Unidirectional control has obvious shortcomings: for example, an excitatory opsin alone can only raise the firing rate of target neurons, not lower it or even clamp to a baseline level.
This setup would also be unable to *lower* the firing rate quickly, in the case of a dynamic reference trajectory.
Moreover, besides enabling bidirectional stimulation, general multi-input methods could enable more precise control by targeting different neurons simultaneously, such as stimulation of cells at different depths in a cortical column or across columns.

### Advantages of model-based, optimal control
While a previous study [@newman15] has already laid the foundation for bidirectional CLOC, it does not feature the generalizability and scalability of the model-based, optimal control algorithms introduced by later work [@bolus21] (see @sec-prev-work) for unidirectional actuation.
This adaptive linear-quadratic regulator (LQR) approach is more robust to disturbances and can scale to multi-input multi-output (MIMO) systems.
Moreover, its behavior can be easily configured by setting penalties on state error, the control signal, and even the derivative of the control signal to encourage smooth actuation.

### Challenges of combining
Thus, a natural goal for furthering CLOC is to combine the advantages of multi-input/bidirectional actuation and model-based optimal control---however, this poses additional challenges and opportunities.
The adaptive LQR method previously developed is unsuitable for multi-input actuation because it does not model the constraint that the input (light intensity) must be nonnegative. 
While violations of this constraint are relatively infrequent and of little consequence when using an excitatory opsin to reach elevated, slowly varying trajectories, a dual-input scenario would be different.
For instance, the controller might call for negative inhibitory input rather than positive excitatory input---a problem that might be solvable with heuristics for the simplest cases but which would pose serious limitations with increasing actuator count.

### Innovation
I propose addressing this problem using model predictive control (MPC), which is widely used for its flexibility in implementing optimal control with constraints.
Rather than computing the control signal from the current error signal at each step, MPC looks ahead, optimizing over the predicted trajectory some finite number of steps into the future, in what is known as "receding horizon control."
I hypothesize that a model predictive strategy will be able to optimize multi-input optogenetic actuation while accommodating experimental constraints and considerations and maintaining low error levels during fast, real-time control.
I plan to achieve this aim by adapting previously demonstrated linear models and adding input constraints, demonstrating the advantage of MPC-powered multi-input CLOC *in silico*, and benchmarking algorithm performance to inform future *in-vivo* experiments.

:::{#fig-mpc}
::::{.dark-invert}
![](img/mpc-schematic.png){style='padding:.5em; background: white'}
::::

An illustration of how MPC optimizes the system input over a receding horizon.
By [Martin Behrendt](https://en.wikipedia.org/wiki/File:MPC_scheme_basic.svg), licensed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en).
:::

## Approach
### System and controller formulation
Naturally, the model is a vital element of MPC.
I will use a previously developed Gaussian linear dynamical system (GLDS) model [@bolus21], which has been shown to reliably capture firing rate dynamics in a light-driven spiking neuron system.
The discrete-time GLDS is governed by the following equations:

$$ x_{t + 1} = Ax_{t} + Bu_{t} + w_t\ , $$

$$ y_{t} = Cx_{t} + d\ , $$

$$ z_{t} = y_{t} + v_t\ , $$

where $x_{t} \in \RR^n$ is the $n$-dimensional state, $u_{t} \in \RR^k$ is the $k$-dimensional stimulus (i.e., $k = 2$ for two opsins, one light source each), $y_{t} \in \RR^m$ is the firing rate in spikes/timestep (for each of $m$ measured neurons), and $z_{t} \in \RR^m$ is the number of binned spikes observed at time $t$.
$A \in \RR^{n \times n}$, $B \in \RR^{n \times k}$, and $C \in \RR^{m \times n}$ are the state transition, input, and output matrices, respectively.
$w_{t} \sim \mathcal{N}\left( 0, Q \right)$ and $v_{t}\mathcal{\sim N}\left( 0, R \right)$ are Gaussian-distributed process and measurement noise, respectively, and $d \in \RR^{m \times 1}$ represents baseline firing rates.
Model order ($n$) and horizon length ($T \in \NN$) will be chosen to balance complexity and prediction error for noise-driven fitting data generated from the test network.
The latent state $x_{t}$ will be estimated online using the Kalman filter [@kalman60], driven by the prediction error $z_{t} - {\widehat{y}}_{t|t - 1}$.

I will set hard non-negative constraints on the light input as well as a ceiling determined by hardware limitations (i.e., the maximum voltage deliverable to the LED driver).
To design an appropriate cost function, I will use a conventional per-time step quadratic form

$$\ell( x_{t},r_{t},u_{t} ) = ( x_{t} - r_{t} )^{T}Q^{\text{ctrl}}( x_{t} - r_{t} ) + u^{T}R^{\text{ctrl}}u\ ,$$

where $r_{t} \in \RR^n$ is the reference trajectory at time $t$.
$Q^{\text{ctrl}}$ and $R^{\text{ctrl}}$ are real $n \times n$ and $k \times k$ matrices chosen to appropriately penalize tracking error and stimulus size, respectively.
This quadratic cost function formulation lends the problem well to standard optimization techniques---combined with a linear dynamical system, it constitutes the classical linear-quadratic-Gaussian (LQG) control problem.

Then, at every time step $t$ the controller solves the following quadratic program:

$$ 
\begin{aligned}
    \mini{} \quad & \sum_{\tau=t}^{t+T} \ell(x_\tau, u_\tau) \\
    \text{subject to} \quad & u_\tau \succeq 0 \\
        & x_{\tau + 1} = Ax_{\tau}+Bu_\tau \\
\end{aligned}
$$

where $T \in \NN$ is the number of steps in the prediction/control horizon and $\succeq$ indicates an inequality for each element of $u_\tau \in \RR^k$.
This yields the solution $\tilde{u}_\tau,...,\tilde{u}_{\tau+T-1}$, of which we take just the first step to apply to the system:

$$ u_t = \tilde{u}_t $$


### Demonstration of advantages *in silico*
To demonstrate the advantages of  control and of MPC, I will control the firing rate of simulated spiking neurons under four conditions: LQR with one opsin, LQR with two opsins, MPC with one opsin and MPC with two opsins.
I will test scenarios where the limitations of unidirectional control and LQR will be manifest, namely clamping activity to baseline levels in the presence of unmodeled disturbances to the system and for time-varying reference trajectories.
I will also demonstrate the more advanced multi-input applications of MPC by controlling multiple neurons simultaneously with multiple light sources.

All experiments will be performed on a simulated randomly connected network of excitatory and inhibitory leaky integrate-and-fire (LIF) neurons or a Poisson linear dynamical system [@macke11] model fit to simulated data.
The framework of Aim 1 will be leveraged to simulate electrode recording and optogenetic stimulation, and spikes from individual neurons will be used as inputs to the controller.
In all experiments I will compare my MPC approach to the unconstrained LQR controller developed previously [@bolus21].
I will also compare to an optimal open-loop stimulus computed over a whole-trial horizon.
To evaluate controller performance, I will use metrics such as the mean-squared error (MSE) between the reference firing rate and the Gaussian window-smoothed firing rate of the spikes received by the controller during the trial.
Noise will be provided to the network where needed to simulate an external disturbance.

### Preparation for real-time experiments
To inform future experiments where compute time is crucial for control performance, I will record compute time of LQR and MPC approaches for varying numbers of input and output channels $k$ and $m$. 
For instance, the time required to solve the quadratic program for MPC would provide a preliminary estimate of the minimum control period we may want to use when implementing MPC in real time.
To get a better idea of how this compute latency might affect a real experiment, I will also test multi-input MPC on a more realistic simulation, such as one of the example experiments from @sec-cleo-experiments, leveraging the latency simulation capabilities of the CLOC simulation framework.

## Expected results
I expect that bidirectional control will be perform better than unidirectional control at the aforementioned tasks of clamping baseline activities and following a dynamic reference trajectory.
I also expect that MPC will attain higher performance than LQR for all multi-input conditions and especially in the case of a dynamic reference.
While MPC will be considerably slower than LQR, it should be fast enough for control of neural phenomena on the timescale of hundreds of milliseconds.

## Preliminary results
Basic simulations controlling a linear dynamical system model fit to experimental data show the advantages of bidirectional control and of MPC (see @fig-mpc-sim).
Bidirectional actuation allows the system to avoid overshooting the reference, in the case of LQR, or to minimize error faster by first exciting then inhibiting, in the case of MPC.
MPC's advantages in looking ahead also clearly allow it to follow the reference more closely than the heuristic LQR controller (assigning negative inputs to the second light source).

```{julia}
using JuMP
using Plots
using LinearAlgebra

#defining all parameters for model and probem
A = [1 -6.66e-13 -2.03e-9 -4.14e-6;
        9.83e-4 1 -4.09e-8 -8.32e-5;
        4.83e-7 9.83e-4 1 -5.34e-4;
        1.58e-10 4.83e-7 9.83e-4 .9994;
]

B = [9.83e-4 4.83e-7 1.58e-10 3.89e-14]'
B1 = B
# fake opsin
Binh = -B .* (1 .+ randn(4, 1)./5)
B2 = [B Binh]

C = [-.0096 .0135 .005 -.0095]

y2z(y) = exp(61.4*y - 5.468)
z2y(z) = (log(z) + 5.468)/61.4

t_step = 0.001 #1 milisecond
pred = 25 #prediction horizon
ctr = 5 #control horizon
sample = 200 #number of steps between sampling points for control

zD = 0.2
yD = z2y(zD)
uD = inv(C*inv(I - A)*B) * yD
#xD = inv(I - A) * B * uD

Q = C'*C
R = 1e-5*I

T = 13000
Tpred = pred*sample
# add in our variable reference
#structured to take a list of firing rate values, convert them to x vectors, then pad with zeros if necessary
##zref = [i < 3000 ? 0.1 : 0.15 for i in 1:T] #trial firing rate reference
# trial firing rate reference
zref = .1 .+ .08*sin.(range(start=0, stop=2*pi, length=Int(1.5*Tpred)));

#zref = [0.2 for i in 1:(T)] #trial firing rate reference
```

```{julia}
using MatrixEquations: ared

function lqr(T, x0, zref, nu=1)
    if nu == 1
        B = B1
    elseif nu == 2
        B = B2
    end

    P, _, _ = ared(A, B1, R, Q)
    K = inv(R + B1'*P*B1) * B1'*P*A

    zs = zeros(1, T)
    us = zeros(nu, T)
    x = x0
    zs[:, 1] = y2z.(C*x)
    for t in 1:T
        zref_current = t > length(zref) ? zref[end] : zref[t]
        yD = z2y(zref_current)
        uD = inv(C*inv(I - A)*B1) * yD
        xD = inv(I - A) * B1 * uD

        u = (-K*(x - xD) + uD)
        if nu == 2
            # use other opsin if negative
            u = u[1] < 0 ? [0, -u[1]] : [u[1], 0]
        elseif nu == 1 && u[1] < 0
            u = [0]
        end


        us[:, t] = u
        x = A*x + B*u
        # println("-->>🦄🌈🤡🥳🎉🎊🪅<<--")
        y = C*x
        zs[:, t] = y2z.(y)
    end
    return zs, us
end

lqr1res = lqr(T, zeros(4), zref, 1);
lqr2res = lqr(T, zeros(4), zref, 2);
# plot(plotctrl(lqr1res...), plotctrl(lqr2res...))
```

```{julia}
using OSQP

function mpc(steps, x0, zref; nu=1, u_clamp=nothing, sample=250)
    if nu == 1
        B = B1
    elseif nu == 2
        B = B2
    end

    Tpred = pred*sample
    Tall = steps*sample
    if length(zref) < Tall + Tpred
        zrefpad = cat(zref, fill(zref[end], Tall + Tpred - length(zref)), dims=1)
    end

    neuron = Model(OSQP.Optimizer)
    set_silent(neuron)

    #Define state variables
    @variables(neuron, begin
        x[i=1:4, t=1:Tpred]
        0 ≤ u[1:nu, 1:(ctr+1)]
        yD[i=1:1, t=1:Tpred]
        # xD[i=1:4, t=1:Tpred], (start = 0)
    end)

    @expressions(
        neuron,
        begin
            y, C*x
            # x_error[t=1:Tpred], x[:, t] - xD[:, t]
            # x_cost[t=1:Tpred], x_error[t]'*Q*x_error[t]
            y_error[t=1:Tpred], y[:, t] - yD[:, t]
            y_cost[t=1:Tpred], y_error'[t]*y_error[t]
            # sampled_x_cost[t=1:pred], x_cost[t*sample]
            sampled_y_cost[t=1:pred], y_cost[t*sample]
            u_cost[t=1:ctr+1], u[t]'*R*u[t]
        end
    )

    #fix first sample steps
    @constraint(neuron, x[:, 2:(sample)] .== A*x[:, 1:sample-1] + B*u[:, 1])

    #fix each sample period
    for i in 1:(ctr-1)
        @constraint(neuron, x[:, (sample*i+1):(sample*i+sample)] .== A*x[:, (sample*i):(sample*i+sample-1)] + B*u[:, (i+1)])
    end

    #fix rest of inputs 
    @constraint(neuron, x[:, (ctr*sample+1):(Tpred)] .== A*x[:, (ctr*sample):(Tpred-1)] + B*u[:, (ctr+1)])

    yDall = z2y.(zrefpad)

    # x_cost[t] returns a 1x1 matrix, which we need to index to get the value out
    # J = @objective(neuron, Min, sum(sampled_x_cost[t] for t in 2:(pred)) + sum(u_cost[t] for t in 1:ctr+1))
    J = @objective(neuron, Min, sum(sampled_y_cost[t] for t in 2:(pred)) + sum(u_cost[t] for t in 1:ctr+1))

    # if nu == 2 
    #     B = [B -B]
    # end
    zs = zeros(1, steps*sample)
    us = zeros(nu, steps*sample)
    x_current = x0
    tfine = 1
    for t in 1:steps
        fix.(x[:, 1], x_current; force=true)
        if u_clamp != nothing
            fix.(u[:, 1], u_clamp; force=true)
        end
        #now need to update the reference by "shifting it" one sample size forward and padding with end value
        fix.(yD[:], yDall[tfine:tfine+Tpred-1], force=true)

        optimize!(neuron)
        zs[1, tfine] = y2z.(value(y[1]))
        us[:, tfine] = value.(u[:, 1])
        # append!(zs , y2z.(value(y[1])))
        # append!(us, value(u[:, 1]))
        x_current = value.(x[:, 2])
        # println(x_current)
        # now effectively apply optimal first u for 250 more steps
        const_u = value.(u[:, 1]);
        tfine += 1
        for i in 1:(sample-1)
            x_current = A*x_current + B*const_u
            # append!(zs, y2z.((C*x_current)[1]))
            zs[1, tfine] = y2z.((C*x_current)[1])
            us[:, tfine] = const_u
            tfine += 1
        end
    end
    #println(solution_summary(neuron))
    return zs, us
end

steps = T ÷ sample
mpc2res = mpc(steps, zeros(4), zref, nu=2, u_clamp=nothing, sample=sample);
```
```{julia}
mpc1res = mpc(steps, zeros(4), zref, nu=1, u_clamp=nothing, sample=sample);
```

```{julia}
#| label: fig-mpc-sim
#| fig-cap: Simulated control of an linear dynamical system with 1- and 2-input control, using LQR and MPC controllers. The top panel of each contains the reference and the actual firing rate, in spikes/second. The bottom contains the light intensity, in terms of mW/mm^2^, where blue represents light for an excitatory opsin (such as ChR2) and red-orange that for an inhibitory opsin (such as Jaws). 
using LaTeXStrings
function plotctrl(zs, us; title=nothing, plotargs...)
    nu = size(us, 1)
    last = zref[end][1]
    if (length(zref)) < T
        for i in (length(zref)):(T - 1)
            append!(zref, last)
        end
    end

    # print(length(zref), "\n", length(us), "\n", length(zs))
    # print("zref: $(length(zref))")

    time = (1:T) ./ 1000
    zs_plot = plot(time, [zref zs'], label=[L"r" L"z"],
        color=["green" "black"], lw=2)
    plot!(legend=false)
    if title != nothing
        plot!(title=title)
    end

    # if nu == 1
    #     ucolor = :lightskyblue
    # elseif nu == 2
    #     ucolor = [:lightskyblue :orangered]
    # end
    # ucolor = "lightskyblue" if nu == 1 else [:lightskyblue :redorange] end
    u_plot = plot(time, us[1, :], color="#72b5f2", lw=2, xlabel="time (s)", legend=false, label=L"u_{exc}")
    if nu == 2
        plot!(time, us[2, :], color=:orangered, lw=2, label=L"u_{inh}")
    end

    return plot(zs_plot, u_plot; layout=(2, 1), link=:x, plotargs...)
end

lqr1plot = plotctrl(lqr1res...; title="1-input LQR")
lqr2plot = plotctrl(lqr2res...; title="2-input LQR", legend=:topright, legendcolumns=2)
mpc1plot = plotctrl(mpc1res...; title="1-input MPC")
mpc2plot = plotctrl(mpc2res...; title="2-input MPC")

plot(lqr1plot, lqr2plot, mpc1plot, mpc2plot, layout=4)
```

## Potential pitfalls & alternative strategies {#sec-aim2-pitfalls}
There are some limitations in the proposed GLDS model that may need to be addressed.
While it was adequate for the experiments in @bolus21, a Poisson linear dynamical system model [@macke11] may be needed in the case that negative predicted firing rates (because the output is not constrained to be nonnegative) produce significant model mismatch error.
Also, in case the data is not fit well by a GLDS, I may explore the impact of making parts of the model nonlinear.
The output equation $y=Cx+d$, for example, could be replaced by a nonlinear equation of the form $y=f(x)$.
While this would likely make the estimation of $x$ more expensive, the underlying dynamics could remain linear, leaving the same underlying quadratic program for the controller to solve.

This touches another potential concern: the speed of the algorithm.
If the optimization problem takes too long to solve, there are a few options to explore.
One is that some variations in the control scheme can help balance speed and performance, such as letting the control horizon be shorter than the prediction horizon, which shrinks the optimization problem.
Likewise, the control period can be longer than the time step of the system, reducing how often the control signal is computed.
If conventional methods such as these are unsuccessful, I may turn to methods such as that described in @wang10 or training an artificial neural network to approximate the exact solution of the quadratic program or to minimize the cost directly.
Another potential solution is explicit MPC [@bemporad02], which finds a piecewise-affine explicit solution to the quadratic program which can be faster than using a solver to obtain the implicit solution for small-enough problems.
